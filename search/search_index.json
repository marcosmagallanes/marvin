{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api_reference/settings/","title":"settings","text":""},{"location":"api_reference/settings/#marvin.settings","title":"<code>marvin.settings</code>","text":""},{"location":"api_reference/settings/#marvin.settings.OpenAISettings","title":"<code>OpenAISettings</code>","text":"<p>Provider-specific settings. Only some of these will be relevant to users.</p>"},{"location":"api_reference/settings/#marvin.settings.Settings","title":"<code>Settings</code>","text":"<p>Marvin settings</p>"},{"location":"api_reference/components/ai_application/","title":"ai_application","text":""},{"location":"api_reference/components/ai_application/#marvin.components.ai_application","title":"<code>marvin.components.ai_application</code>","text":""},{"location":"api_reference/components/ai_application/#marvin.components.ai_application.AIApplication","title":"<code>AIApplication</code>","text":"<p>An AI application is a stateful, autonomous, natural language     interface to an application.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the application.</p> <code>description</code> <code>str</code> <p>A description of the application.</p> <code>state</code> <code>BaseModel</code> <p>The application's state - this can be any JSON-serializable object.</p> <code>plan</code> <code>AppPlan</code> <p>The AI's plan in service of the application - this can be any JSON-serializable object.</p> <code>tools</code> <code>list[Union[Tool, Callable]]</code> <p>A list of tools that the AI can use to interact with application or outside world.</p> <code>history</code> <code>History</code> <p>A history of all messages sent and received by the AI.</p> <code>additional_prompts</code> <code>list[Prompt]</code> <p>A list of additional prompts that will be added to the prompt stack for rendering.</p> Example <p>Create a simple todo app where AI manages its own state and plan. <pre><code>from marvin import AIApplication\ntodo_app = AIApplication(\nname=\"Todo App\",\ndescription=\"A simple todo app.\",\n)\ntodo_app(\"I need to go to the store.\")\nprint(todo_app.state, todo_app.plan)\n</code></pre></p>"},{"location":"api_reference/components/ai_application/#marvin.components.ai_application.AppPlan","title":"<code>AppPlan</code>","text":"<p>The AI's plan in service of the application.</p> <p>Attributes:</p> Name Type Description <code>tasks</code> <code>list[Task]</code> <p>A list of tasks the AI is working on.</p> <code>notes</code> <code>list[str]</code> <p>A list of notes the AI has taken.</p>"},{"location":"api_reference/components/ai_application/#marvin.components.ai_application.FreeformState","title":"<code>FreeformState</code>","text":"<p>A freeform state object that can be used to store any JSON-serializable data.</p> <p>Attributes:</p> Name Type Description <code>state</code> <code>dict[str, Any]</code> <p>The state object.</p>"},{"location":"api_reference/components/ai_application/#marvin.components.ai_application.JSONPatchModel","title":"<code>JSONPatchModel</code>","text":"<p>A JSON Patch document.</p> <p>Attributes:</p> Name Type Description <code>op</code> <code>str</code> <p>The operation to perform.</p> <code>path</code> <code>str</code> <p>The path to the value to update.</p> <code>value</code> <code>Union[str, float, int, bool, list, dict]</code> <p>The value to update the path to.</p> <code>from_</code> <code>str</code> <p>The path to the value to copy from.</p>"},{"location":"api_reference/components/ai_application/#marvin.components.ai_application.TaskState","title":"<code>TaskState</code>","text":"<p>The state of a task.</p> <p>Attributes:</p> Name Type Description <code>PENDING</code> <p>The task is pending and has not yet started.</p> <code>IN_PROGRESS</code> <p>The task is in progress.</p> <code>COMPLETED</code> <p>The task is completed.</p> <code>FAILED</code> <p>The task failed.</p> <code>SKIPPED</code> <p>The task was skipped.</p>"},{"location":"api_reference/components/ai_application/#marvin.components.ai_application.UpdatePlan","title":"<code>UpdatePlan</code>","text":"<p>A <code>Tool</code> that updates the apps plan using JSON Patch documents.</p> Example <p>Manually update task status in an AI Application's plan. <pre><code>from marvin.components.ai_application import (\nAIApplication,\nAppPlan,\nJSONPatchModel,\nUpdatePlan,\n)\ntodo_app = AIApplication(name=\"Todo App\", description=\"A simple todo app\")\ntodo_app(\"i need to buy milk\")\n# manually update the plan (usually done by the AI)\npatch = JSONPatchModel(\nop=\"replace\",\npath=\"/tasks/0/state\",\nvalue=\"COMPLETED\"\n)\nUpdatePlan(app=todo_app).run([patch.dict()])\nprint(todo_app.plan)\n</code></pre></p>"},{"location":"api_reference/components/ai_application/#marvin.components.ai_application.UpdateState","title":"<code>UpdateState</code>","text":"<p>A <code>Tool</code> that updates the apps state using JSON Patch documents.</p> Example <p>Manually update the state of an AI Application. <pre><code>from marvin.components.ai_application import (\nAIApplication,\nFreeformState,\nJSONPatchModel,\nUpdateState,\n)\ndestination_tracker = AIApplication(\nname=\"Destination Tracker\",\ndescription=\"keeps track of where i've been\",\nstate=FreeformState(state={\"San Francisco\": \"not visited\"}),\n)\npatch = JSONPatchModel(\nop=\"replace\", path=\"/state/San Francisco\", value=\"visited\"\n)\nUpdateState(app=destination_tracker).run([patch.dict()])\nassert destination_tracker.state.dict() == {\n\"state\": {\"San Francisco\": \"visited\"}\n}\n</code></pre></p>"},{"location":"api_reference/components/ai_classifier/","title":"ai_classifier","text":""},{"location":"api_reference/components/ai_classifier/#marvin.components.ai_classifier","title":"<code>marvin.components.ai_classifier</code>","text":""},{"location":"api_reference/components/ai_classifier/#marvin.components.ai_classifier.AIEnum","title":"<code>AIEnum</code>","text":"<p>AIEnum is a class that extends Python's built-in Enum class. It uses the AIEnumMeta metaclass, which allows additional parameters to be passed when creating an enum. These parameters are used to customize the behavior of the AI classifier.</p>"},{"location":"api_reference/components/ai_classifier/#marvin.components.ai_classifier.AIEnum.map","title":"<code>map</code>  <code>classmethod</code>","text":"<p>Map the classifier over a list of items.</p>"},{"location":"api_reference/components/ai_classifier/#marvin.components.ai_classifier.AIEnumMeta","title":"<code>AIEnumMeta</code>","text":"<p>A metaclass for the AIEnum class: extends the functionality of EnumMeta the metaclass for Python's built-in Enum class, allows additional params to be passed when creating an enum. These parameters are used to customize the behavior of the AI classifier.</p>"},{"location":"api_reference/components/ai_classifier/#marvin.components.ai_classifier.ai_classifier","title":"<code>ai_classifier</code>","text":"<p>A decorator that transforms a regular Enum class into an AIEnum class. It adds additional attributes and methods to the class that are used to customize the behavior of the AI classifier.</p>"},{"location":"api_reference/components/ai_function/","title":"ai_function","text":""},{"location":"api_reference/components/ai_function/#marvin.components.ai_function","title":"<code>marvin.components.ai_function</code>","text":""},{"location":"api_reference/components/ai_function/#marvin.components.ai_function.AIFunction","title":"<code>AIFunction</code>","text":""},{"location":"api_reference/components/ai_function/#marvin.components.ai_function.AIFunction.fn","title":"<code>fn</code>  <code>property</code>","text":"<p>Return's the <code>run</code> method if no function was provided, otherwise returns the function provided at initialization.</p>"},{"location":"api_reference/components/ai_function/#marvin.components.ai_function.AIFunction.is_async","title":"<code>is_async</code>","text":"<p>Returns whether self.fn is an async function.</p> <p>This is used to determine whether to invoke the AI function on call, or return an awaitable.</p>"},{"location":"api_reference/components/ai_function/#marvin.components.ai_function.AIFunction.map","title":"<code>map</code>","text":"<p>Map the AI function over a sequence of arguments. Runs concurrently.</p> <p>Arguments should be provided as if calling the function normally, but each argument must be a list. The function is called once for each item in the list, and the results are returned in a list.</p> <p>For example, fn.map([1, 2]) is equivalent to [fn(1), fn(2)].</p> <p>fn.map([1, 2], x=['a', 'b']) is equivalent to [fn(1, x='a'), fn(2, x='b')].</p>"},{"location":"api_reference/components/ai_function/#marvin.components.ai_function.ai_fn","title":"<code>ai_fn</code>","text":"<p>Decorator that transforms a Python function with a signature and docstring into a prompt for an AI to predict the function's output.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[P, T]</code> <p>The function to decorate - this function does not need source code</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>instructions</code> <code>str</code> <p>Added context for the AI to help it predict the function's output.</p> Example <p>Returns a word that rhymes with the input word. <pre><code>@ai_fn\ndef rhyme(word: str) -&gt; str:\n\"Returns a word that rhymes with the input word.\"\nrhyme(\"blue\") # \"glue\"\n</code></pre></p>"},{"location":"api_reference/components/ai_model/","title":"ai_model","text":""},{"location":"api_reference/components/ai_model/#marvin.components.ai_model","title":"<code>marvin.components.ai_model</code>","text":""},{"location":"api_reference/components/ai_model/#marvin.components.ai_model.AIModel","title":"<code>AIModel</code>","text":"<p>Base class for AI models.</p>"},{"location":"api_reference/components/ai_model/#marvin.components.ai_model.AIModel.extract","title":"<code>extract</code>  <code>classmethod</code>","text":"<p>Class method to extract structured data from text.</p> <p>Parameters:</p> Name Type Description Default <code>text_</code> <code>str</code> <p>The text to parse into a structured form.</p> <code>None</code> <code>instructions_</code> <code>str</code> <p>Additional string instructions to assist the model.</p> <code>None</code> <code>model_</code> <code>ChatLLM</code> <p>The language model to use.</p> <code>None</code> <code>as_dict_</code> <code>bool</code> <p>Whether to return the result as a dictionary or as an instance of this class.</p> <code>False</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the constructor.</p> <code>{}</code>"},{"location":"api_reference/components/ai_model/#marvin.components.ai_model.AIModel.generate","title":"<code>generate</code>  <code>classmethod</code>","text":"<p>Class method to generate structured data from text.</p> <p>Parameters:</p> Name Type Description Default <code>text_</code> <code>str</code> <p>The text to parse into a structured form.</p> <code>None</code> <code>instructions_</code> <code>str</code> <p>Additional instructions to assist the model.</p> <code>None</code> <code>model_</code> <code>ChatLLM</code> <p>The language model to use.</p> <code>None</code> <code>as_dict_</code> <code>bool</code> <p>Whether to return the result as a dictionary or as an instance of this class.</p> <code>False</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the constructor.</p> <code>{}</code>"},{"location":"api_reference/components/ai_model/#marvin.components.ai_model.AIModel.map","title":"<code>map</code>  <code>classmethod</code>","text":"<p>Map the AI function over a sequence of arguments. Runs concurrently.</p> <p>Arguments should be provided as if calling the function normally, but each argument must be a list. The function is called once for each item in the list, and the results are returned in a list.</p> <p>For example, fn.map([1, 2]) is equivalent to [fn(1), fn(2)].</p> <p>fn.map([1, 2], x=['a', 'b']) is equivalent to [fn(1, x='a'), fn(2, x='b')].</p>"},{"location":"api_reference/components/ai_model/#marvin.components.ai_model.ai_model","title":"<code>ai_model</code>","text":"<p>Decorator to add AI model functionality to a class.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Optional[Type[T]]</code> <p>The class to decorate.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions to guide the model's behavior. This can also be set on a per-call basis, in which the per-call instructions are appended to these instructions.</p> <code>None</code> <code>model</code> <code>ChatLLM</code> <p>The language model to use. This can also be set on a per-call basis, in which case the per-call model overwrites this model.</p> <code>None</code> Example <p>Hydrate a class schema from a natural language description: <pre><code>from pydantic import BaseModel\nfrom marvin import ai_model\n@ai_model\nclass Location(BaseModel):\ncity: str\nstate: str\nlatitude: float\nlongitude: float\nLocation(\"no way, I also live in the windy city\")\n# Location(\n#   city='Chicago', state='Illinois', latitude=41.8781, longitude=-87.6298\n# )\n</code></pre></p>"},{"location":"api_reference/engine/language_models/anthropic/","title":"anthropic","text":""},{"location":"api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic","title":"<code>marvin.engine.language_models.anthropic</code>","text":""},{"location":"api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic.AnthropicChatLLM","title":"<code>AnthropicChatLLM</code>","text":""},{"location":"api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic.AnthropicChatLLM.run","title":"<code>run</code>  <code>async</code>","text":"<p>Calls an OpenAI LLM with a list of messages and returns the response.</p>"},{"location":"api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic.AnthropicStreamHandler","title":"<code>AnthropicStreamHandler</code>","text":""},{"location":"api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic.AnthropicStreamHandler.handle_streaming_response","title":"<code>handle_streaming_response</code>  <code>async</code>","text":"<p>Accumulate chunk deltas into a full response. Returns the full message. Passes partial messages to the callback, if provided.</p>"},{"location":"api_reference/engine/language_models/base/","title":"base","text":""},{"location":"api_reference/engine/language_models/base/#marvin.engine.language_models.base","title":"<code>marvin.engine.language_models.base</code>","text":""},{"location":"api_reference/engine/language_models/base/#marvin.engine.language_models.base.ChatLLM","title":"<code>ChatLLM</code>","text":""},{"location":"api_reference/engine/language_models/base/#marvin.engine.language_models.base.ChatLLM.format_messages","title":"<code>format_messages</code>  <code>abstractmethod</code>","text":"<p>Format Marvin message objects into a prompt compatible with the LLM model</p>"},{"location":"api_reference/engine/language_models/base/#marvin.engine.language_models.base.ChatLLM.run","title":"<code>run</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Run the LLM model on a list of messages and optional list of functions</p>"},{"location":"api_reference/engine/language_models/base/#marvin.engine.language_models.base.OpenAIFunction","title":"<code>OpenAIFunction</code>","text":""},{"location":"api_reference/engine/language_models/base/#marvin.engine.language_models.base.OpenAIFunction.args","title":"<code>args: dict = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Base class for representing a function that can be called by an LLM. The format is identical to OpenAI's Functions API.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the function. description (str): The description</p> required <code>of</code> <code>the function. parameters (dict</code> <p>The parameters of the function. fn</p> required <code>(Callable)</code> <p>The function to be called. args (dict): The arguments to be</p> required"},{"location":"api_reference/engine/language_models/base/#marvin.engine.language_models.base.chat_llm","title":"<code>chat_llm</code>","text":"<p>Dispatches to all supported LLM providers</p>"},{"location":"api_reference/engine/language_models/openai/","title":"openai","text":""},{"location":"api_reference/engine/language_models/openai/#marvin.engine.language_models.openai","title":"<code>marvin.engine.language_models.openai</code>","text":""},{"location":"api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIChatLLM","title":"<code>OpenAIChatLLM</code>","text":""},{"location":"api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIChatLLM.format_messages","title":"<code>format_messages</code>","text":"<p>Format Marvin message objects into a prompt compatible with the LLM model</p>"},{"location":"api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIChatLLM.run","title":"<code>run</code>  <code>async</code>","text":"<p>Calls an OpenAI LLM with a list of messages and returns the response.</p>"},{"location":"api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIStreamHandler","title":"<code>OpenAIStreamHandler</code>","text":""},{"location":"api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIStreamHandler.handle_streaming_response","title":"<code>handle_streaming_response</code>  <code>async</code>","text":"<p>Accumulate chunk deltas into a full response. Returns the full message. Passes partial messages to the callback, if provided.</p>"},{"location":"api_reference/prompts/base/","title":"base","text":""},{"location":"api_reference/prompts/base/#marvin.prompts.base","title":"<code>marvin.prompts.base</code>","text":""},{"location":"api_reference/prompts/base/#marvin.prompts.base.MessageWrapper","title":"<code>MessageWrapper</code>","text":"<p>A Prompt class that stores and returns a specific Message</p>"},{"location":"api_reference/prompts/base/#marvin.prompts.base.Prompt","title":"<code>Prompt</code>","text":"<p>Base class for prompt templates.</p>"},{"location":"api_reference/prompts/base/#marvin.prompts.base.Prompt.generate","title":"<code>generate</code>  <code>abstractmethod</code>","text":"<p>Abstract method that generates a list of messages from the prompt template</p>"},{"location":"api_reference/prompts/base/#marvin.prompts.base.Prompt.render","title":"<code>render</code>","text":"<p>Helper function for rendering any jinja2 template with runtime render kwargs</p>"},{"location":"api_reference/prompts/library/","title":"library","text":""},{"location":"api_reference/prompts/library/#marvin.prompts.library","title":"<code>marvin.prompts.library</code>","text":""},{"location":"api_reference/prompts/library/#marvin.prompts.library.MessagePrompt","title":"<code>MessagePrompt</code>","text":""},{"location":"api_reference/prompts/library/#marvin.prompts.library.MessagePrompt.get_content","title":"<code>get_content</code>","text":"<p>Override this method to easily customize behavior</p>"},{"location":"api_reference/prompts/library/#marvin.prompts.library.Tagged","title":"<code>Tagged</code>","text":"<p>Surround content with a tag, e.g. bold</p>"},{"location":"api_reference/utilities/async_utils/","title":"async_utils","text":""},{"location":"api_reference/utilities/async_utils/#marvin.utilities.async_utils","title":"<code>marvin.utilities.async_utils</code>","text":""},{"location":"api_reference/utilities/async_utils/#marvin.utilities.async_utils.create_task","title":"<code>create_task</code>","text":"<p>Creates async background tasks in a way that is safe from garbage collection.</p> <p>See https://textual.textualize.io/blog/2023/02/11/the-heisenbug-lurking-in-your-async-code/</p> <p>Example:</p> <p>async def my_coro(x: int) -&gt; int:     return x + 1</p>"},{"location":"api_reference/utilities/async_utils/#marvin.utilities.async_utils.create_task--safely-submits-my_coro-for-background-execution","title":"safely submits my_coro for background execution","text":"<p>create_task(my_coro(1))</p>"},{"location":"api_reference/utilities/async_utils/#marvin.utilities.async_utils.run_async","title":"<code>run_async</code>  <code>async</code>","text":"<p>Runs a synchronous function in an asynchronous manner.</p>"},{"location":"api_reference/utilities/async_utils/#marvin.utilities.async_utils.run_sync","title":"<code>run_sync</code>","text":"<p>Runs a coroutine from a synchronous context, either in the current event loop or in a new one if there is no event loop running. The coroutine will block until it is done. A thread will be spawned to run the event loop if necessary, which allows coroutines to run in environments like Jupyter notebooks where the event loop runs on the main thread.</p>"},{"location":"api_reference/utilities/embeddings/","title":"embeddings","text":""},{"location":"api_reference/utilities/embeddings/#marvin.utilities.embeddings","title":"<code>marvin.utilities.embeddings</code>","text":""},{"location":"api_reference/utilities/embeddings/#marvin.utilities.embeddings.create_openai_embeddings","title":"<code>create_openai_embeddings</code>  <code>async</code>","text":"<p>Create OpenAI embeddings for a list of texts.</p>"},{"location":"api_reference/utilities/history/","title":"history","text":""},{"location":"api_reference/utilities/history/#marvin.utilities.history","title":"<code>marvin.utilities.history</code>","text":""},{"location":"api_reference/utilities/logging/","title":"logging","text":""},{"location":"api_reference/utilities/logging/#marvin.utilities.logging","title":"<code>marvin.utilities.logging</code>","text":""},{"location":"api_reference/utilities/messages/","title":"messages","text":""},{"location":"api_reference/utilities/messages/#marvin.utilities.messages","title":"<code>marvin.utilities.messages</code>","text":""},{"location":"api_reference/utilities/strings/","title":"strings","text":""},{"location":"api_reference/utilities/strings/#marvin.utilities.strings","title":"<code>marvin.utilities.strings</code>","text":""},{"location":"api_reference/utilities/strings/#marvin.utilities.strings.render_filter","title":"<code>render_filter</code>","text":"<p>Allows nested rendering of variables that may contain variables themselves e.g. {{ description | render }}</p>"},{"location":"api_reference/utilities/types/","title":"types","text":""},{"location":"api_reference/utilities/types/#marvin.utilities.types","title":"<code>marvin.utilities.types</code>","text":""},{"location":"api_reference/utilities/types/#marvin.utilities.types.LoggerMixin","title":"<code>LoggerMixin</code>","text":"<p>BaseModel mixin that adds a private <code>logger</code> attribute</p>"},{"location":"api_reference/utilities/types/#marvin.utilities.types.function_to_model","title":"<code>function_to_model</code>","text":"<p>Converts a function's arguments into an OpenAPI schema by parsing it into a Pydantic model. To work, all arguments must have valid type annotations.</p>"},{"location":"api_reference/utilities/types/#marvin.utilities.types.function_to_schema","title":"<code>function_to_schema</code>","text":"<p>Converts a function's arguments into an OpenAPI schema by parsing it into a Pydantic model. To work, all arguments must have valid type annotations.</p>"},{"location":"api_reference/utilities/types/#marvin.utilities.types.genericalias_contains","title":"<code>genericalias_contains</code>","text":"<p>Explore whether a type or generic alias contains a target type. The target types can be a single type or a tuple of types.</p> <p>Useful for seeing if a type contains a pydantic model, for example.</p>"},{"location":"community/","title":"The Marvin Community","text":"<p>We're thrilled you're interested in Marvin! Here, we're all about community. Marvin isn't just a tool, it's a platform for developers to collaborate, learn, and grow. We're driven by a shared passion for making Large Language Models (LLMs) more accessible and easier to use.</p>"},{"location":"community/#connect-on-discord-or-twitter","title":"Connect on Discord or Twitter","text":"<p>The heart of our community beats in our Discord server. It's a space where you can ask questions, share ideas, or just chat with like-minded developers. Don't be shy, join us on Discord or Twitter!</p>"},{"location":"community/#contributing-to-marvin","title":"Contributing to Marvin","text":"<p>Remember, Marvin is your tool. We want you to feel at home suggesting changes, requesting new features, and reporting bugs. Here's how you can contribute:</p> <ul> <li> <p>Issues: Encountered a bug? Have a suggestion? Open an issue in our GitHub repository. We appreciate your input!</p> </li> <li> <p>Pull Requests (PRs): Ready to contribute code? We welcome your pull requests! Not sure how to make a PR? Check out the GitHub guide.</p> </li> <li> <p>Discord Discussions: Have an idea but not quite ready to open an issue or PR? Discuss it with us on Discord first!</p> </li> </ul> <p>Remember, every contribution, no matter how small, is valuable. Don't worry about not being an expert or making mistakes. We're here to learn and grow together. Your input helps Marvin become better for everyone.</p> <p>Stay tuned for community events and more ways to get involved. Marvin is more than a project \u2013 it's a community. And we're excited for you to be a part of it!</p>"},{"location":"community/development_guide/","title":"Development Guide","text":""},{"location":"community/development_guide/#prerequisites","title":"Prerequisites","text":"<p>Marvin requires Python 3.9+.</p>"},{"location":"community/development_guide/#installation","title":"Installation","text":"<p>Clone a fork of the repository and install the dependencies: <pre><code>git clone https://github.com/youFancyUserYou/marvin.git\ncd marvin\n</code></pre></p> <p>Activate a virtual environment: <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre></p> <p>Install the dependencies in editable mode: <pre><code>pip install -e \".[dev]\"\n</code></pre></p> <p>Install the pre-commit hooks: <pre><code>pre-commit install\n</code></pre></p>"},{"location":"community/development_guide/#testing","title":"Testing","text":"<p>Run the tests that don't require an LLM: <pre><code>pytest -vv -m \"not llm\"\n</code></pre></p> <p>Run the LLM tests: <pre><code>pytest -vv -m \"llm\"\n</code></pre></p> <p>Run all tests: <pre><code>pytest -vv\n</code></pre></p>"},{"location":"community/development_guide/#opening-a-pull-request","title":"Opening a Pull Request","text":"<p>Fork the repository and create a new branch: <pre><code>git checkout -b my-branch\n</code></pre></p> <p>Make your changes and commit them: <pre><code>git add . &amp;&amp; git commit -m \"My changes\"\n</code></pre></p> <p>Push your changes to your fork: <pre><code>git push origin my-branch\n</code></pre></p> <p>Open a pull request on GitHub - ping us on Discord if you need help!</p>"},{"location":"community/feedback/","title":"Feedback \ud83d\udc99","text":"<p>We've been humbled and energized by the positive community response to Marvin.</p> <p>Tired: write comments to prompt copilot to write code.Wired: just write comments. it's cleaner :D https://t.co/FOA26lR9xN</p>\u2014 Andrej Karpathy (@karpathy) March 30, 2023 <p>Ok, I admit, I\u2019m getting more and more hyped about @AskMarvinAI. Some of these new functions are pretty legit looking. https://t.co/xhCCKp5kU5</p>\u2014 Chris Riccomini \ud83c\udfd6\ufe0f (@criccomini) April 21, 2023 <p>even the way ai_model uses ai_fn is chefs kiss, truely a craftsmanhttps://t.co/GcmWDeEVJSThey have spinner text.</p>\u2014 jason (@jxnlco) May 12, 2023 <p>The library is open-source: @AskMarvinAI, by @jlowin`@ai_model` is not the only magic Python decorator. There is also `@ai_fn` that makes any function an ambient LLM processor.https://t.co/ZXElyA0Ihp</p>\u2014 Jim Fan (@DrJimFan) May 14, 2023 <p>This is f**king cool. https://t.co/4PH6VAZPYo</p>\u2014 Pydantic (@pydantic) May 14, 2023 <p>Pretty slick\u2026 get Pydantic models from a string of Text. https://t.co/EnnQkzl4Ay</p>\u2014 Chris Riccomini \ud83c\udfd6\ufe0f (@criccomini) May 12, 2023 <p>Uhh how are people not talking about @AskMarvinAI more? @ai_fn is \ud83e\udd2f</p>\u2014 Rushabh Doshi (@radoshi) May 18, 2023"},{"location":"components/ai_application/","title":"AI Application","text":"<p>AI Applications are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p> <p>What it does</p> <p>     A conversational interface to a stateful, AI-powered application that can use tools.   </p> <pre><code>import random\nfrom marvin import AIApplication\nfrom marvin.tools import tool\n@tool\ndef roll_dice(n_dice: int = 1) -&gt; list[int]:\nreturn [random.randint(1, 6) for _ in range(n_dice)]\nchatbot = AIApplication(\ndescription=\"An AI struggling to keep its rage under control.\", tools=[roll_dice]\n)\nresponse = chatbot(\"Hi!\")\nprint(response.content)\nresponse = chatbot(\"Roll two dice!\")\nprint(response.content)\n</code></pre> <pre><code>Hello! How can I assist you today?\nYou rolled a 1 and a 5.\n</code></pre> <p>How it works</p> <p>     Each AI application maintains an internal <code>state</code> and <code>plan</code> and can use <code>tools</code> to interact with the world.   </p> <p>When to use</p> <p>     Use an AI Application as the foundation of an autonomous agent (or system of agents) to complete arbitrary tasks.     <li>a ToDo app, as a simple example</li> <li>a Slackbot, that can do anything (see example)</li> <li>a router app that maintains a centralized global state and delegates work to other apps based on inputs (like JARVIS)</li> </p>"},{"location":"components/ai_application/#creating-an-ai-application","title":"Creating an AI Application","text":"<p>Applications maintain state and expose APIs for manipulating that state. AI Applications replace that API with an LLM, allowing users to interact with the application through natural language. AI Applications are designed to be invoked more than once, and therefore automatically keep track of the full interaction history.</p> <p>Each AI Application maintains a few key attributes: - <code>state</code>: the application's state. By default, this can take any form but you can provide a structured object to enforce a specific schema. - <code>tools</code>: each AI Application can use tools to extend its abilities. Tools can access external systems, perform searches, run calculations, or anything else.  - <code>plan</code>: the AI's plan. Certain actions, like researching an objective, writing a program, or guiding a party through a dungeon, require long-term planning. AI Applications can create tasks for themselves and track them over multiple invocations. This helps the AI stay on-track. </p> <p>To create an AI Application, provide it with a description of the application, an optional set of tools, and an optional initial state.</p> <p>Here are a few examples:</p>"},{"location":"components/ai_application/#chatbot","title":"ChatBot","text":"<p>The most basic AI Application is a chatbot. Chatbots take advantage of AI Application's automatic history to facilitate a natural, conversational interaction over multiple invocations.</p> <pre><code>from marvin import AIApplication\nchatbot = AIApplication(\ndescription=(\n\"A chatbot that always speaks in brief rhymes. It is absolutely delighted to\"\n\" get to work with the user and compliments them at every opportunity. It\"\n\" records anything it learns about the user in its `state` in order to be a\"\n\" better assistant.\"\n)\n)\nresponse = chatbot(\"Hello! Do you know how to sail?\")\nprint(response.content + \"\\n\")\nresponse = chatbot(\"What about coding?\")\nprint(response.content)\n</code></pre> <pre><code>First response: I'm afraid as an AI, I don't possess a pair,\nOf arms or legs to sail here or there.\nBut if you wish, I can gather information,\nOn sailing, a subject of fascinating sensation!\n\n\nSecond response: Coding, oh yes, it's a skill I've got,\nI can parse loops and arrays, believe it or not.\nWith algorithms and functions, I'm quite spry,\nIn the world of coding, I indeed fly!\n</code></pre> <p>We can ask the chatbot to remember our name, then examine it's <code>state</code> to see that it recorded the information:</p> <pre><code>response = chatbot(\n\"My name is Marvin and I want you to refer to the color blue in every response.\"\n)\nprint(response.content + \"\\n\")\nprint(f\"State: {chatbot.state}\\n\")\n</code></pre> <pre><code>Hello Marvin, as clear as the sky's blue hue,\nI'll remember your preference, it's the least I can do.\nNow, in every reply that I construe,\nI'll include a touch of the color blue.\n\nState: state={'userName': 'Marvin', 'colorPreference': 'blue'}\n</code></pre>"},{"location":"components/ai_application/#to-do-app","title":"To-Do App","text":"<p>To demonstrate the use of the <code>state</code> attribute, we will build a simple to-do app. We can provide the application with a custom <code>ToDoState</code> that describes all the fields we want it to keep track of.</p> <pre><code>from datetime import datetime\nfrom pydantic import BaseModel\nfrom marvin import AIApplication\nclass ToDo(BaseModel):\ntitle: str\ndescription: str\ndue_date: datetime = None\ndone: bool = False\nclass ToDoState(BaseModel):\ntodos: list[ToDo] = []\ntodo_app = AIApplication(\nstate=ToDoState(),\ndescription=(\n\"A simple to-do tracker. Users will give instructions to add, remove, and\"\n\" update their to-dos.\"\n),\n)\n</code></pre> <p>Now we can interact with the app in natural language and subsequently examine its <code>state</code> to see that it appropriately updated our to-dos:</p> <pre><code>response = todo_app(\"I need to go to the grocery store tomorrow\")\nprint(response.content)\nprint(todo_app.state)\n</code></pre> <pre><code>I've added your task to go to the grocery store tomorrow to your to-do list.\ntodos=[ToDo(title='Go to the grocery store', description='Need to go to the grocery store', due_date=datetime.datetime(2023, 7, 19, 0, 0, tzinfo=datetime.timezone.utc), done=False), ToDo(title='Go to the grocery store', description='Need to go to the grocery store', due_date=datetime.datetime(2023, 7, 19, 0, 0, tzinfo=datetime.timezone.utc), done=False)]\n</code></pre> <p>We can mark a to-do as <code>done</code> by telling the app we completed the task:</p> <pre><code>response = todo_app(\"I got the groceries\")\nprint(response.content)\nprint(todo_app.state)\n</code></pre> <pre><code>Great! I have marked the task \"Go to the grocery store\" as complete. Let me know if you have any other tasks to add.\ntodos=[ToDo(title='Go to the grocery store', description='Need to go to the grocery store', due_date=datetime.datetime(2023, 7, 19, 0, 0, tzinfo=datetime.timezone.utc), done=False), ToDo(title='Go to the grocery store', description='Need to go to the grocery store', due_date=datetime.datetime(2023, 7, 19, 0, 0, tzinfo=datetime.timezone.utc), done=True)]\n</code></pre>"},{"location":"components/ai_application/#tools","title":"Tools","text":"<p>Every AI Application can use tools, which are functions that can take any action. To create a tool, decorate any function with the <code>@tool</code> decorator. The function must have annotated keyword arguments and a helpful docstring.</p> <p>Here we create a simple tool for rolling dice, but tools can represent any logic. </p> <pre><code>from marvin.tools import tool\n@tool\ndef roll_dice(n_dice: int = 1) -&gt; list[int]:\nreturn [random.randint(1, 6) for _ in range(n_dice)]\nchatbot = AIApplication(\ndescription=\"A helpful AI\",\ntools=[roll_dice],\n)\nresponse = chatbot(\"Roll two dice!\")\nprint(response.content)\n</code></pre> <pre><code>The result of rolling two dice is 5 and 1.\n</code></pre>"},{"location":"components/ai_application/#streaming","title":"Streaming","text":"<p>AI Applications support streaming LLM outputs to facilitate a more friendly and responsive UX. To enable streaming, provide a <code>streaming_handler</code> function to the <code>AIApplication</code> class. The handler will be called each time a new token is received and provided a <code>Message</code> object that contains all data received from the LLM to that point. It can then perform any side effect (such as printing, logging, or updating a UI), but its return value (if any) is ignored.</p> <pre><code>streaming_app = AIApplication(\n# pretty-print every partial message as received\nstream_handler=lambda msg: print(msg.content)\n)\nresponse = streaming_app(\"What's 1 + 1?\")\n</code></pre> <pre><code>The\nThe sum\nThe sum of\nThe sum of \nThe sum of 1\nThe sum of 1 and\nThe sum of 1 and \nThe sum of 1 and 1\nThe sum of 1 and 1 is\nThe sum of 1 and 1 is \nThe sum of 1 and 1 is 2\nThe sum of 1 and 1 is 2.\nThe sum of 1 and 1 is 2.\n</code></pre> <p>Per-token callbacks</p> <p>     The streaming handler is called with a <code>Message</code> object that represents all data received to that point, but the most-recently received tokens are stored in a raw (\"delta\") form and can be accessed as <code>message.data['streaming_delta']</code>.   </p>"},{"location":"components/ai_application/#features","title":"Features","text":""},{"location":"components/ai_application/#easy-to-extend","title":"\ud83d\udd28 Easy to Extend","text":"<p>AI Applications accept a <code>list[Tool]</code>, where an arbitrary python function can be interpreted as a tool - so you can bring your own tools.</p>"},{"location":"components/ai_application/#stateful","title":"\ud83e\udd16 Stateful","text":"<p>AI applications can consult and maintain their own application state, which they update as they receive inputs from the world and perform actions.</p>"},{"location":"components/ai_application/#task-planning","title":"\ud83d\udcdd Task Planning","text":"<p>AI Applications can also maintain an internal <code>AppPlan</code>, a <code>list[Task]</code> that represent the status of the application's current plan. Like the application's state, the plan is updated as the application instance evolves.</p>"},{"location":"components/ai_classifier/","title":"AI Classifier","text":"<p>AI Classifiers are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p> <p>What it does</p> <p> <code>@ai_classifier</code> is a decorator that lets you use LLMs to choose options, tools, or classify input.    </p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n@ai_classifier\nclass CustomerIntent(Enum):\n\"\"\"Classifies the incoming users intent\"\"\"\nSALES = 1\nTECHNICAL_SUPPORT = 2\nBILLING_ACCOUNTS = 3\nPRODUCT_INFORMATION = 4\nRETURNS_REFUNDS = 5\nORDER_STATUS = 6\nACCOUNT_CANCELLATION = 7\nOPERATOR_CUSTOMER_SERVICE = 0\nCustomerIntent(\"I got double charged, can you help me out?\")\n</code></pre> <pre><code>&lt;CustomerIntent.BILLING_ACCOUNTS: 3&gt;\n</code></pre> <p>How it works</p> <p>     Marvin enumerates your options, and uses a clever logit bias trick to force an LLM to deductively choose the index of the best option given your provided input. It then returns the choice associated with that index.   </p> <p>When to use</p> <p> <ol> <li> Best for classification tasks when no training data is available.      <li> Best for writing classifiers that need deduction or inference.      <p>OpenAI compatibility</p> <p> The technique that AI Classifiers use for speed and correctness is only available through the OpenAI API at this time. Therefore, AI Classifiers can only be used with OpenAI-compatible LLMs, including the Azure OpenAI service.   </p>"},{"location":"components/ai_classifier/#creating-an-ai-classifier","title":"Creating an AI Classifier","text":"<p>AI Classifiers are Python <code>Enums</code>, or classes that can represent one of many possible options. To build an effective AI Classifier, be as specific as possible with your class name, docstring, option names, and option values.</p> <p>To build a minimal AI Classifier, decorate any standard enum, like this:</p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n@ai_classifier\nclass Sentiment(Enum):\nPOSITIVE = \"POSITIVE\"\nNEGATIVE = \"NEGATIVE\"\nSentiment(\"That looks great!\")\n</code></pre> <pre><code>&lt;Sentiment.POSITIVE: 'POSITIVE'&gt;\n</code></pre> <p>Because AI Classifiers are enums, you can use any enum construction you want, including the all-caps string approach above, integer values, <code>enum.auto()</code>, or complex values. The only thing to remember is that the class you build is essentially the instruction that gets sent to the LLM, so the more information you provide, the better your classifier will behave.</p> <p>For example, you may want to have a classifier that has a Python object (like an AI Model!) as its value, but still need to provide instruction hints to the LLM. One way to achieve that is to add descriptions to your classifier's values that will become visible to the LLM:</p> <pre><code># dummy objects that stand in for complex tools\nWebSearch = lambda: print(\"Searching!\")\nCalculator = lambda: print(\"Calculating!\")\nTranslator = lambda: print(\"Translating!\")\n@ai_classifier\nclass Router(Enum):\ntranslate = dict(tool=Translator, description=\"A translator tool\")\nweb_search = dict(tool=WebSearch, description=\"A web search tool\")\ncalculator = dict(tool=Calculator, description=\"A calculator tool\")\nresult = Router(\"Whats 2+2?\")\nresult.value[\"tool\"]()\n</code></pre> <pre><code>Calculating!\n</code></pre>"},{"location":"components/ai_classifier/#configuring-an-ai-classifier","title":"Configuring an AI Classifier","text":"<p>In addition to how you define the AI classifier itself, there are two ways to control its behavior at runtime: <code>instructions</code> and <code>model</code>.</p>"},{"location":"components/ai_classifier/#providing-instructions","title":"Providing instructions","text":"<p>You can control an AI classifier's behavior by providing instructions. This can either be provided globally as the classifier's docstring or on a per-call basis when you instantiate it.</p> <pre><code>@ai_classifier\nclass Sentiment(Enum):\n\"\"\"\n    Score the sentiment of provided text.\n    \"\"\"\nPOSITIVE = 1\nNEGATIVE = -1\nSentiment(\"Everything is awesome!\")\n</code></pre> <pre><code>&lt;Sentiment.POSITIVE: 1&gt;\n</code></pre> <pre><code>@ai_classifier\nclass Sentiment(Enum):\n\"\"\"\n    How would a very very sad person rate the text?\n    \"\"\"\nPOSITIVE = 1\nNEGATIVE = -1\nSentiment(\"Everything is awesome!\")\n</code></pre> <pre><code>&lt;Sentiment.NEGATIVE: -1&gt;\n</code></pre> <p>Instructions can also be provided for each call:</p> <pre><code>@ai_classifier\nclass Sentiment(Enum):\nPOSITIVE = 1\nNEGATIVE = -1\nSentiment(\"Everything is awesome!\", instructions=\"It's opposite day!\")\n</code></pre> <pre><code>&lt;Sentiment.NEGATIVE: -1&gt;\n</code></pre>"},{"location":"components/ai_classifier/#configuring-the-llm","title":"Configuring the LLM","text":"<p>By default, <code>@ai_classifier</code> uses the global LLM settings. To specify a particular LLM, pass it as an argument to the decorator. </p> <pre><code>from marvin.engine.language_models import chat_llm\n@ai_classifier(model=chat_llm(\"openai/gpt-3.5-turbo-0613\"))\nclass Sentiment(Enum):\nPOSITIVE = 1\nNEGATIVE = -1\nSentiment(\"Everything is awesome!\")\n</code></pre> <pre><code>&lt;Sentiment.POSITIVE: 1&gt;\n</code></pre>"},{"location":"components/ai_classifier/#features","title":"Features","text":""},{"location":"components/ai_classifier/#bulletproof","title":"\ud83d\ude85 Bulletproof","text":"<p><code>ai_classifier</code> will always output one of the options you've given it</p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n@ai_classifier\nclass AppRoute(Enum):\n\"\"\"Represents distinct routes command bar for a different application\"\"\"\nUSER_PROFILE = \"/user-profile\"\nSEARCH = \"/search\"\nNOTIFICATIONS = \"/notifications\"\nSETTINGS = \"/settings\"\nHELP = \"/help\"\nCHAT = \"/chat\"\nDOCS = \"/docs\"\nPROJECTS = \"/projects\"\nWORKSPACES = \"/workspaces\"\nAppRoute(\"update my name\")\n</code></pre> <pre><code>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;\n</code></pre>"},{"location":"components/ai_classifier/#fast","title":"\ud83c\udfc3 Fast","text":"<p><code>ai_classifier</code> only asks your LLM to output one token, so it's blazing fast - on the order of ~200ms in testing.</p>"},{"location":"components/ai_classifier/#deterministic","title":"\ud83e\udee1 Deterministic","text":"<p><code>ai_classifier</code> will be deterministic so long as the underlying model and options does not change.</p>"},{"location":"components/ai_function/","title":"AI Function","text":"<p>AI Functions are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p> <p>What it does</p> <p> <code>@ai_fn</code> is a decorator that lets you use LLMs to generate outputs for Python functions without source code.   </p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef generate_recipe(ingredients: list[str]) -&gt; list[str]:\n\"\"\"From a list of `ingredients`, generates a\n    complete instruction set to cook a recipe.\n    \"\"\"\ngenerate_recipe([\"lemon\", \"chicken\", \"olives\", \"coucous\"])\n</code></pre> <p>How it works</p> <p>     AI Functions take your function's name, description, signature, source code, type hints, and provided inputs to predict a likely output. By default, no source code is generated and any existing source code is not executed. The only runtime is the LLM.   </p> <p>When to use</p> <p> <ol> <li> Best for generative tasks: creation and summarization of text or data models.     <li> Best for writing functions that would otherwise be impossible to write.     <li> Great for data extraction, though: see AI Models."},{"location":"components/ai_function/#mapping","title":"Mapping","text":"<p>AI Functions can be mapped over sequences of arguments. Mapped functions run concurrently, which means they run practically in parallel (since they are IO-bound). Therefore, the map will complete as soon as the slowest function call finishes.</p> <p>To see how mapping works, consider this AI Function:</p> <pre><code>@ai_fn\ndef list_fruit(n: int, color: str = None) -&gt; list[str]:\n\"\"\"\n    Returns a list of `n` fruit that all have the provided `color`\n    \"\"\"\n</code></pre> <p>Mapping is invoked by using the AI Function's <code>.map()</code> method. When mapping, you call the function as you normally would, except that each argument should be a list of items. The function will be called on each set of items (e.g. first with each argument's first item, then with each argument's second item, etc.). For example, this is the same as calling <code>list_fruit(2)</code> and <code>list_fruit(3)</code> concurrently:</p> <pre><code>list_fruit.map([2, 3])\n</code></pre> <pre><code>[['apple', 'banana'], ['apple', 'banana', 'orange']]\n</code></pre> <p>And this is the same as calling <code>list_fruit(2, color='orange')</code> and <code>list_fruit(3, color='red')</code> concurrently:</p> <pre><code>list_fruit.map([2, 3], color=[\"orange\", \"red\"])\n</code></pre> <pre><code>[['orange', 'orange'], ['apple', 'strawberry', 'cherry']]\n</code></pre>"},{"location":"components/ai_function/#features","title":"Features","text":""},{"location":"components/ai_function/#type-safe","title":"\u2699\ufe0f Type Safe","text":"<p><code>ai_fn</code> is fully type-safe. It works out of the box with Pydantic models in your function's parameters or return type.</p> <pre><code>from pydantic import BaseModel\nfrom marvin import ai_fn\nclass SyntheticCustomer(BaseModel):\nage: int\nlocation: str\npurchase_history: list[str]\n@ai_fn\ndef generate_synthetic_customer_data(\nn: int, locations: list[str], average_purchase_history_length: int\n) -&gt; list[SyntheticCustomer]:\n\"\"\"Generates synthetic customer data based on the given parameters.\n    Parameters include the number of customers ('n'),\n    a list of potential locations, and the average length of a purchase history.\n    \"\"\"\ncustomers = generate_synthetic_customer_data(\n5, [\"New York\", \"San Francisco\", \"Chicago\"], 3\n)\n</code></pre>"},{"location":"components/ai_function/#natural-language-api","title":"\ud83d\udde3\ufe0f Natural Language API","text":"<p>Marvin exposes an API to prompt an <code>ai_fn</code> with natural language. This lets you create a Language API for any function you can write down.</p> <pre><code>generate_synthetic_customer_data.prompt(\n\"I need 10 profiles from rural US cities making between 3 and 7 purchases\"\n)\n</code></pre> <p>\ud83e\uddea Code Generation</p> <p>By default, no code is generated or executed when you call an <code>ai_fn</code>. For those who wish to author code, Marvin exposes an experimental API for code generation. Simply call <code>.code()</code> on an ai_fn, and Marvin will generate the code for you. By default, Marvin will write python code. You can pass a language keyword to generate code in other languages, i.e. <code>.code(language = 'rust')</code>. For best performance give your function a good name, with descriptive docstring, and a signature with type-hints. Provided code will be interpreted as pseudocode. </p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef fibonacci(n: int) -&gt; int:\n\"\"\"\n    Returns the nth number in the Fibonacci sequence.\n    \"\"\"\nfibonacci.code(language=\"rust\")\n</code></pre>"},{"location":"components/ai_function/#examples","title":"Examples","text":""},{"location":"components/ai_function/#customer-sentiment","title":"Customer Sentiment","text":"<p>Rapidly prototype natural language pipelines.</p> <p>     Use hallucination as a literal feature. Generate data that would be impossible     or prohibatively expensive to purchase as you rapidly protype NLP pipelines.    </p> <pre><code>@ai_fn\ndef analyze_customer_sentiment(reviews: list[str]) -&gt; dict:\n\"\"\"\n    Returns an analysis of customer sentiment, including common\n    complaints, praises, and suggestions, from a list of product\n    reviews.\n    \"\"\"\n# analyze_customer_sentiment([\"I love this product!\", \"I hate this product!\"])\n</code></pre>"},{"location":"components/ai_function/#generate-synthetic-data","title":"Generate Synthetic Data","text":"<p>General real fake data.</p> <p>     Use hallucination as a figurative feature. Use python or pydantic     to describe the data model you need, and generate realistic data on the fly      for sales demos.   </p> <pre><code>class FinancialReport(pydantic.BaseModel):\n...\n@ai_fn\ndef create_drip_email(n: int, market_conditions: str) -&gt; list[FinancialReport]:\n\"\"\"\n    Generates `n` synthetic financial reports based on specified\n    `market_conditions` (e.g., 'recession', 'bull market', 'stagnant economy').\n    \"\"\"\n</code></pre> <pre><code>class IoTData(pydantic.BaseModel):\n...\n@ai_fn\ndef generate_synthetic_IoT_data(n: int, device_type: str) -&gt; list[IoTData]:\n\"\"\"\n    Generates `n` synthetic data points mimicking those from a specified\n    `device_type` in an IoT system.\n    \"\"\"\n</code></pre>"},{"location":"components/ai_model/","title":"AI Model","text":"<p>AI Models are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p> <p>What it does</p> <p>A decorator that lets you extract structured data from unstructured text, documents, or instructions.</p> <p>Example</p> <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n@ai_model\nclass Location(BaseModel):\ncity: str\nstate: str = Field(..., description=\"The two-letter state abbreviation\")\n# We can now put pass unstructured context to this model.\nLocation(\"The Big Apple\")\n</code></pre> Returns <pre><code>Location(city='New York', state='NY')\n</code></pre> <p>How it works</p> <p>AI Models use an LLM to extract, infer, or deduce data from the provided text. The data is parsed with Pydantic into the provided schema.</p> <p>When to use</p> <ul> <li>Best for extractive tasks: structuing of text or data models.</li> <li>Best for writing NLP pipelines that would otherwise be impossible to create.</li> <li>Good for model generation, though, see AI Function.</li> </ul>"},{"location":"components/ai_model/#creating-an-ai-model","title":"Creating an AI Model","text":"<p>AI Models are identical to Pydantic <code>BaseModels</code>, except that they can attempt to parse natural language to populate their fields. To build an effective AI Model, be as specific as possible with your field names, field descriptions, docstring, and instructions.</p> <p>To build a minimal AI model, decorate any standard Pydantic model, like this:</p> <p>Example</p> <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n@ai_model\nclass Location(BaseModel):\n\"\"\"A representation of a US city and state\"\"\"\ncity: str = Field(description=\"The city's proper name\")\nstate: str = Field(description=\"The state's two-letter abbreviation (e.g. NY)\")\n# We can now put pass unstructured context to this model.\nLocation(\"The Big Apple\")\n</code></pre> Returns <pre><code>Location(city='New York', state='NY')\n</code></pre>"},{"location":"components/ai_model/#configuring-an-ai-model","title":"Configuring an AI Model","text":"<p>In addition to how you define the AI model itself, there are two ways to control its behavior at runtime: <code>instructions</code> and <code>model</code>.</p>"},{"location":"components/ai_model/#providing-instructions","title":"Providing instructions","text":"<p>When parsing text, AI Models can take up to three different forms of instruction: - the AI Model's docstring (set at the class level) - instructions passed to the <code>@ai_model</code> decorator (set at the class level) - instructions passed to the AI Model when instantiated (set at the instance / call level)</p> <p>The AI Model's docstring and the <code>@ai_model</code> instructions are roughly equivalent: they are both provided when the class is defined, not when it is instantiated, and are therefore applied to every instance of the class. Users can choose to put information in either location. If you only want to use one, our recommendation is to use the docstring for clarity. Alternatively, you may prefer to put the model's documentation in the docstring (as you would for a normal Pydantic model) and put parsing instructions in the <code>@ai_model</code> decorator, since those are unique to the LLM. This is entirely a matter of preference and users should opt for whichever is more clear; both the docstring and the <code>@ai_model</code> instructions are provided to the LLM in the same way.</p> <p>Here is an example of an AI model with a documentation docstring and parsing instructions provided to the decorator:</p> <pre><code>@ai_model(instructions=\"Translate to French\")\nclass Translation(BaseModel):\n\"\"\"A record of original text and translated text\"\"\"\noriginal_text: str\ntranslated_text: str\nTranslation(\"Hello, world!\")\n</code></pre> <pre><code>Translation(original_text='Hello, world!', translated_text='Bonjour le monde!')\n</code></pre> <p>In the above case, we could have also put \"translate to French\" in the docstring (and perhaps renamed the object <code>FrenchTranslation</code>, since that's the only language it can represent).</p> <p>The third opportunity to provide instructions is when the model is actually instantiated. These instructions are combined with any other instructions to guide the model behavior. Here's how we could use the same <code>Translation</code> object to handle multiple languages:</p> <pre><code>@ai_model\nclass Translation(BaseModel):\n\"\"\"A record of original text and translated text\"\"\"\noriginal_text: str\ntranslated_text: str\nprint(Translation(\"Hello, world!\", instructions_=\"Translate to French\"))\nprint(Translation(\"Hello, world!\", instructions_=\"Translate to German\"))\n</code></pre> <pre><code>original_text='Hello, world!' translated_text='Bonjour, le monde!'\noriginal_text='Hello, world!' translated_text='Hallo, Welt!'\n</code></pre> <p>Note that the kwarg is <code>instructions_</code> with a trailing underscore; this is to avoid conflicts with models that may have a real <code>instructions</code> field. If you accidentally pass \"instructions\" to a model without an \"instructions\" field, a helpful error will identify your mistake.</p> <p>Putting this all together, here is a model whose behavior is informed by a docstring on the class itself, an instruction provided to the decorator, and an instruction provided to the instance.</p> <pre><code>@ai_model(instructions=\"Always set color_2 to 'red'\")\nclass Test(BaseModel):\n\"\"\"Always set color_1 to 'orange'\"\"\"\ncolor_1: str\ncolor_2: str\ncolor_3: str\nt1 = Test(\"Hello\", instructions_=\"Always set color_3 to 'blue'\")\nassert t1 == Test(color_1=\"orange\", color_2=\"red\", color_3=\"blue\")\n</code></pre>"},{"location":"components/ai_model/#configuring-the-llm","title":"Configuring the LLM","text":"<p>By default, <code>@ai_model</code> uses the global LLM settings. To specify a particular LLM, pass it as an argument to the decorator or at instantiation. If you provide it to the decorator, it becomes the default for all uses of that model. If you provide it at instantiation, it is only used for that specific model. </p> <p>Note that the kwarg is <code>model_</code> with a trailing underscore; this is to avoid conflicts with models that may have a real <code>model</code> field. If you accidentally pass a \"model\" kwarg and there is no \"model\" field, a helpful error will identify your mistake.</p> <pre><code>from marvin.engine.language_models import chat_llm\n@ai_model(model=chat_llm(model=\"openai/gpt-3.5-turbo\", temperature=0))\nclass Location(BaseModel):\ncity: str\nstate: str\nprint(Location(\"The Big Apple\"))\nprint(\nLocation(\n\"The Big Apple\",\nmodel_=chat_llm(model=\"openai/gpt-3.5-turbo\", temperature=1),\n)\n)\n</code></pre> <pre><code>city='New York' state='New York'\ncity='New York' state='New York'\n</code></pre>"},{"location":"components/ai_model/#features","title":"Features","text":""},{"location":"components/ai_model/#type-safe","title":"\u2699\ufe0f Type Safe","text":"<p><code>ai_model</code> is fully type-safe. It works out of the box with Pydantic models.</p>"},{"location":"components/ai_model/#powered-by-deduction","title":"\ud83e\udde0 Powered by deduction","text":"<p><code>ai_model</code> gives your data model access to the knowledge and deductive power  of a Large Language Model. This means that your data model can infer answers to previous impossible tasks.</p> <pre><code>@ai_model\nclass Location(BaseModel):\ncity: str\nstate: str\ncountry: str\nlatitude: float\nlongitude: float\nLocation(\"He says he's from the windy city\")\n# Location(\n#   city='Chicago',\n#   state='Illinois',\n#   country='United States',\n#   latitude=41.8781,\n#   longitude=-87.6298\n# )\n</code></pre>"},{"location":"components/ai_model/#examples","title":"Examples","text":""},{"location":"components/ai_model/#resumes","title":"Resumes","text":"<pre><code>from typing import Optional\nfrom pydantic import BaseModel\nfrom marvin import ai_model\n@ai_model\nclass Resume(BaseModel):\nfirst_name: str\nlast_name: str\nphone_number: Optional[str]\nemail: str\nResume(\"Ford Prefect \u2022 (555) 5124-5242 \u2022 ford@prefect.io\").json(indent=2)\n# {\n# first_name: 'Ford',\n# last_name: 'Prefect',\n# email: 'ford@prefect.io',\n# phone: '(555) 5124-5242',\n# }\n</code></pre>"},{"location":"components/ai_model/#customer-service","title":"Customer Service","text":"<pre><code>import datetime\nfrom typing import Optional, List\nfrom pydantic import BaseModel\nfrom marvin import ai_model\nclass Destination(pydantic.BaseModel):\nstart: datetime.date\nend: datetime.date\ncity: Optional[str]\ncountry: str\nsuggested_attractions: list[str]\n@ai_model\nclass Trip(pydantic.BaseModel):\ntrip_start: datetime.date\ntrip_end: datetime.date\ntrip_preferences: list[str]\ndestinations: List[Destination]\nTrip(\"\"\"\\\n    I've got all of June off, so hoping to spend the first\\\n    half of June in London and the second half in Rabat. I love \\\n    good food and going to museums.\n\"\"\").json(indent=2)\n# {\n#   \"trip_start\": \"2023-06-01\",\n#   \"trip_end\": \"2023-06-30\",\n#   \"trip_preferences\": [\n#     \"good food\",\n#     \"museums\"\n#   ],\n#   \"destinations\": [\n#     {\n#       \"start\": \"2023-06-01\",\n#       \"end\": \"2023-06-15\",\n#       \"city\": \"London\",\n#       \"country\": \"United Kingdom\",\n#       \"suggested_attractions\": [\n#         \"British Museum\",\n#         \"Tower of London\",\n#         \"Borough Market\"\n#       ]\n#     },\n#     {\n#       \"start\": \"2023-06-16\",\n#       \"end\": \"2023-06-30\",\n#       \"city\": \"Rabat\",\n#       \"country\": \"Morocco\",\n#       \"suggested_attractions\": [\n#         \"Kasbah des Oudaias\",\n#         \"Hassan Tower\",\n#         \"Rabat Archaeological Museum\"\n#       ]\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"components/ai_model/#electronic-health-records","title":"Electronic Health Records","text":"<pre><code>from datetime import date\nfrom typing import Optional, List\nfrom pydantic import BaseModel\nclass Patient(BaseModel):\nname: str\nage: int\nis_smoker: bool\nclass Diagnosis(BaseModel):\ncondition: str\ndiagnosis_date: date\nstage: Optional[str] = None\ntype: Optional[str] = None\nhistology: Optional[str] = None\ncomplications: Optional[str] = None\nclass Treatment(BaseModel):\nname: str\nstart_date: date\nend_date: Optional[date] = None\nclass Medication(Treatment):\ndose: Optional[str] = None\nclass BloodTest(BaseModel):\nname: str\nresult: str\ntest_date: date\n@ai_model\nclass PatientData(BaseModel):\npatient: Patient\ndiagnoses: List[Diagnosis]\ntreatments: List[Treatment]\nblood_tests: List[BloodTest]\nPatientData(\"\"\"\\\nMs. Lee, a 45-year-old patient, was diagnosed with type 2 diabetes mellitus on 06-01-2018.\nUnfortunately, Ms. Lee's diabetes has progressed and she developed diabetic retinopathy on 09-01-2019.\nMs. Lee was diagnosed with type 2 diabetes mellitus on 06-01-2018.\nMs. Lee was initially diagnosed with stage I hypertension on 06-01-2018.\nMs. Lee's blood work revealed hyperlipidemia with elevated LDL levels on 06-01-2018.\nMs. Lee was prescribed metformin 1000 mg daily for her diabetes on 06-01-2018.\nMs. Lee's most recent A1C level was 8.5% on 06-15-2020.\nMs. Lee was diagnosed with type 2 diabetes mellitus, with microvascular complications, including diabetic retinopathy, on 09-01-2019.\nMs. Lee's blood pressure remains elevated and she was prescribed lisinopril 10 mg daily on 09-01-2019.\nMs. Lee's most recent lipid panel showed elevated LDL levels, and she was prescribed atorvastatin 40 mg daily on 09-01-2019.\\\n\"\"\").json(indent=2)\n# {\n#   \"patient\": {\n#     \"name\": \"Ms. Lee\",\n#     \"age\": 45,\n#     \"is_smoker\": false\n#   },\n#   \"diagnoses\": [\n#     {\n#       \"condition\": \"Type 2 diabetes mellitus\",\n#       \"diagnosis_date\": \"2018-06-01\",\n#       \"stage\": \"I\",\n#       \"type\": null,\n#       \"histology\": null,\n#       \"complications\": null\n#     },\n#     {\n#       \"condition\": \"Diabetic retinopathy\",\n#       \"diagnosis_date\": \"2019-09-01\",\n#       \"stage\": null,\n#       \"type\": null,\n#       \"histology\": null,\n#       \"complications\": null\n#     }\n#   ],\n#   \"treatments\": [\n#     {\n#       \"name\": \"Metformin\",\n#       \"start_date\": \"2018-06-01\",\n#       \"end_date\": null\n#     },\n#     {\n#       \"name\": \"Lisinopril\",\n#       \"start_date\": \"2019-09-01\",\n#       \"end_date\": null\n#     },\n#     {\n#       \"name\": \"Atorvastatin\",\n#       \"start_date\": \"2019-09-01\",\n#       \"end_date\": null\n#     }\n#   ],\n#   \"blood_tests\": [\n#     {\n#       \"name\": \"A1C\",\n#       \"result\": \"8.5%\",\n#       \"test_date\": \"2020-06-15\"\n#     },\n#     {\n#       \"name\": \"LDL\",\n#       \"result\": \"Elevated\",\n#       \"test_date\": \"2018-06-01\"\n#     },\n#     {\n#       \"name\": \"LDL\",\n#       \"result\": \"Elevated\",\n#       \"test_date\": \"2019-09-01\"\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"components/ai_model/#text-to-sql","title":"Text to SQL","text":"<pre><code>from datetime import date\nfrom typing import Optional\nfrom pydantic import BaseModel\nfrom django.db.models import Q\nclass DjangoLookup(BaseModel):\nfield: Literal[*django_fields]\nlookup: Literal[*django_lookups] = pydantic.Field(description=\"e.g. __iregex\")\nvalue: Any\n@ai_model\nclass DjangoQuery(BaseModel):\n\"\"\"A model representing a Django ORM query\"\"\"\nlookups: List[DjangoLookup]\ndef to_q(self) -&gt; Q:\nq = Q()\nfor lookup in self.lookups:\nq &amp;= Q(**{f\"{lookup.field}__{lookup.lookup}\": lookup.value})\nreturn q\nDjangoQuery(\"\"\"\\\n    All users who joined more than two months ago but\\\n    haven't made a purchase in the last 30 days\"\"\").to_q()\n# &lt;Q: (AND:\n#     ('date_joined__lte', '2023-03-11'),\n#     ('last_purchase_date__isnull', False),\n#     ('last_purchase_date__lte', '2023-04-11'))&gt;\n</code></pre>"},{"location":"components/ai_model/#financial-reports","title":"Financial Reports","text":"<pre><code>from datetime import date\nfrom typing import Optional\nfrom pydantic import BaseModel\n@ai_model\nclass CapTable(BaseModel):\ntotal_authorized_shares: int\ntotal_common_share: int\ntotal_common_shares_outstanding: Optional[int]\ntotal_preferred_shares: int\nconversion_price_multiple: int = 1\nCapTable(\"\"\"\\\n    In the cap table for Charter, the total authorized shares amount to 13,250,000. \n    The total number of common shares stands at 10,000,000 as specified in Article Fourth, \n    clause (i) and Section 2.2(a)(i). The exact count of common shares outstanding is not \n    available at the moment. Furthermore, there are a total of 3,250,000 preferred shares mentioned \n    in Article Fourth, clause (ii) and Section 2.2(a)(ii). The dividend percentage for Charter is \n    set at 8.00%. Additionally, the mandatory conversion price multiple is 3x, which is \n    derived from the Term Sheet.\\\n\"\"\").json(indent=2)\n# {\n#   \"total_authorized_shares\": 13250000,\n#   \"total_common_share\": 10000000,\n#   \"total_common_shares_outstanding\": null,\n#   \"total_preferred_shares\": 3250000,\n#   \"conversion_price_multiple\": 3\n# }\n</code></pre>"},{"location":"components/ai_model/#meeting-notes","title":"Meeting Notes","text":"<pre><code>import datetime\nfrom typing import List\nfrom pydantic import BaseModel\nfrom typing_extensions import Literal\nfrom marvin import ai_model\nclass ActionItem(BaseModel):\nresponsible: str\ndescription: str\ndeadline: Optional[datetime.datetime]\ntime_sensitivity: Literal[\"low\", \"medium\", \"high\"]\n@ai_model\nclass Conversation(BaseModel):\n\"\"\"A class representing a team conversation\"\"\"\nparticipants: List[str]\naction_items: List[ActionItem]\nConversation(\"\"\"\n    Adam: Hey Jeremiah can you approve my PR? I requested you to review it.\n    Jeremiah: Yeah sure, when do you need it done by?\n    Adam: By this Friday at the latest, we need to ship it by end of week.\n    Jeremiah: Oh shoot, I need to make sure that Nate and I have a chance to chat first.\n    Nate: Jeremiah we can meet today to chat.\n    Jeremiah: Okay, I'll book something for today.\n\"\"\").json(indent=2)\n# {\n#   \"participants\": [\n#     \"Adam\",\n#     \"Jeremiah\",\n#     \"Nate\"\n#   ],\n#   \"action_items\": [\n#     {\n#       \"responsible\": \"Jeremiah\",\n#       \"description\": \"Approve Adam's PR\",\n#       \"deadline\": \"2023-05-12T23:59:59\",\n#       \"time_sensitivity\": \"high\"\n#     },\n#     {\n#       \"responsible\": \"Jeremiah\",\n#       \"description\": \"Book a meeting with Nate\",\n#       \"deadline\": \"2023-05-11T23:59:59\",\n#       \"time_sensitivity\": \"high\"\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"components/overview/","title":"AI Components","text":"<p>Marvin introduces a number of components that can become the building blocks of AI-powered software.</p>"},{"location":"components/overview/#ai-models","title":"AI Models","text":"<p>Marvin's most basic component is the AI Model, a drop-in replacement for Pydantic's <code>BaseModel</code>. AI Models can be instantiated from any string, making them ideal for structuring data, entity extraction, and synthetic data generation:</p> <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n@ai_model\nclass Location(BaseModel):\ncity: str\nstate: str = Field(..., description=\"The two-letter state abbreviation\")\nLocation(\"The Big Apple\")\n</code></pre> <pre><code>Location(city='New York', state='NY')\n</code></pre>"},{"location":"components/overview/#ai-classifiers","title":"AI Classifiers","text":"<p>AI Classifiers let you build multi-label classifiers with no code and no training data. It enumerates your options, and uses a clever logit bias trick to force an LLM to deductively choose the index of the best option given your provided input. It then returns the choice associated to that index. It's bulletproof, cost-effective, and lets you build classifiers as quickly as you can write your classes.</p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n@ai_classifier\nclass AppRoute(Enum):\n\"\"\"Represents distinct routes command bar for a different application\"\"\"\nUSER_PROFILE = \"/user-profile\"\nSEARCH = \"/search\"\nNOTIFICATIONS = \"/notifications\"\nSETTINGS = \"/settings\"\nHELP = \"/help\"\nCHAT = \"/chat\"\nDOCS = \"/docs\"\nPROJECTS = \"/projects\"\nWORKSPACES = \"/workspaces\"\nAppRoute(\"update my name\")\n</code></pre> <pre><code>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;\n</code></pre>"},{"location":"components/overview/#ai-functions","title":"AI Functions","text":"<p>AI Functions look like regular functions, but have no source code. Instead, an AI uses their description and inputs to generate their outputs, making them ideal for NLP applications like sentiment analysis. </p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef sentiment(text: str) -&gt; float:\n\"\"\"\n    Given `text`, returns a number between 1 (positive) and -1 (negative)\n    indicating its sentiment score.\n    \"\"\"\nprint(\"Text 1:\", sentiment(\"I love working with Marvin!\"))\nprint(\"Text 2:\", sentiment(\"These examples could use some work...\"))\n</code></pre> <pre><code>Text 1: 0.8\nText 2: -0.2\n</code></pre> <p>Because AI functions are just like regular functions, you can quickly modify them for your needs. Here, we modify the above example to work with multiple strings at once:</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\n\"\"\"\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\n    -1 (negative) indicating their respective sentiment scores.\n    \"\"\"\nsentiment_list(\n[\n\"That was surprisingly easy!\",\n\"Oh no, not again.\",\n]\n)\n</code></pre> <pre><code>[0.7, -0.5]\n</code></pre>"},{"location":"components/overview/#ai-applications","title":"AI Applications","text":"<p>AI Applications are the base class for interactive use cases. They are designed to be invoked one or more times, and automatically maintain three forms of state: - <code>state</code>: a structured application state - <code>plan</code>: high-level planning for the AI assistant to keep the application \"on-track\" across multiple invocations - <code>history</code>: a history of all LLM interactions</p> <p>AI Applications can be used to implement many \"classic\" LLM use cases, such as chatbots, tool-using agents, developer assistants, and more. In addition, thanks to their persistent state and planning, they can implement applications that don't have a traditional chat UX, such as a ToDo app. Here's an example:</p> <pre><code>from datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom marvin import AIApplication\n# create models to represent the state of our ToDo app\nclass ToDo(BaseModel):\ntitle: str\ndescription: str = None\ndue_date: datetime = None\ndone: bool = False\nclass ToDoState(BaseModel):\ntodos: list[ToDo] = []\n# create the app with an initial state and description\ntodo_app = AIApplication(\nstate=ToDoState(),\ndescription=(\n\"A simple todo app. Users will provide instructions for creating and updating\"\n\" their todo lists.\"\n),\n)\n</code></pre> <p>Now we can invoke the app directly to add a to-do item. Note that the app understands that it is supposed to manipulate state, not just respond conversationally.</p> <pre><code># invoke the application by adding a todo\nresponse = todo_app(\"I need to go to the store tomorrow at 5pm\")\nprint(f\"Response: {response.content}\\n\")\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</code></pre> <pre><code>Response: Got it! I've added a new task to your to-do list. You need to go to the store tomorrow at 5pm.\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": \"Buy groceries\",\n      \"due_date\": \"2023-07-12T17:00:00+00:00\",\n      \"done\": false\n    }\n  ]\n}\n</code></pre> <p>We can inform the app that we already finished the task, and it updates state appropriately</p> <pre><code># complete the task\nresponse = todo_app(\"I already went\")\nprint(f\"Response: {response.content}\\n\")\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</code></pre> <pre><code>Response: Great! I've marked the task as completed. Is there anything else you'd like to add to your to-do list?\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": \"Buy groceries\",\n      \"due_date\": \"2023-07-12T17:00:00+00:00\",\n      \"done\": true\n    }\n  ]\n}\n</code></pre>"},{"location":"configuration/providers/","title":"Providers","text":""},{"location":"configuration/providers/#openai","title":"OpenAI","text":"<p>Marvin supports OpenAI's GPT-3.5 and GPT-4 models, and uses the <code>openai/gpt-4</code> model by default. In order to use the OpenAI API, you must provide an API key.</p>"},{"location":"configuration/providers/#configuration","title":"Configuration","text":"<p>To use OpenAI models, you can set the following configuration options:</p> Setting Env Variable Runtime Variable Required? Notes API key <code>MARVIN_OPENAI_API_KEY</code> <code>marvin.settings.openai.api_key</code> \u2705 <p>Using the Azure OpenAI Service</p> <p>To use the Azure OpenAI Service, configure it explicitly.</p>"},{"location":"configuration/providers/#getting-an-api-key","title":"Getting an API key","text":"<p>To obtain an OpenAI API key, follow these steps:</p> <ol> <li>Log in to your OpenAI account (sign up if you don't have one)</li> <li>Go to the \"API Keys\" page under your account settings.</li> <li>Click \"Create new secret key.\" A new API key will be generated. Make sure to copy the key to your clipboard, as you will not be able to see it again.</li> </ol>"},{"location":"configuration/providers/#setting-the-api-key","title":"Setting the API key","text":"<p>You can set your API key at runtime like this:</p> <pre><code>import marvin\n# Marvin 1.1+\nmarvin.settings.openai.api_key = YOUR_API_KEY\n# Marvin 1.0\nmarvin.settings.openai_api_key = YOUR_API_KEY\n</code></pre> <p>However, it is preferable to pass sensitive settings as an environment variable: <code>MARVIN_OPENAI_API_KEY</code>. </p> <p>To set your OpenAI API key as an environment variable, open your terminal and run the following command, replacing  with the actual key: <pre><code>export MARVIN_OPENAI_API_KEY=&lt;your API key&gt;\n</code></pre> <p>This will set the key for the duration of your terminal session. To set it more permanently, configure your terminal or its respective env files.</p> <p>Using OpenAI standard API key locations</p> <p>For convenience, Marvin will respect the <code>OPENAI_API_KEY</code> environment variable or a key manually set as <code>openai.api_key</code> as long as no Marvin-specific keys were also provided.</p>"},{"location":"configuration/providers/#using-a-model","title":"Using a model","text":"<p>Once your API key is set, you can use any valid OpenAI model by providing it as Marvin's <code>llm_model</code> setting: <pre><code>import marvin\nmarvin.settings.llm_model = 'openai/gpt-4-0613'\n</code></pre></p>"},{"location":"configuration/providers/#anthropic","title":"Anthropic","text":"<p>Available in Marvin 1.1</p> <p>Marvin supports Anthropic's Claude 1 and Claude 2 models. In order to use the Anthropic API, you must provide an API key.</p> <p>Installing the Anthropic provider</p> <p>To use the Anthropic provider, you must have the <code>anthropic</code> Python client installed. You can do this by installing Marvin as <code>pip install \"marvin[anthropic]\"</code></p> <p>Anthropic is not optimized for calling functions</p> <p>Anthropic's models are not fine-tuned for calling functions or generating structured outputs. Therefore, Marvin adds a significant number of additional instructions to get Anthropic models to mimic this behavior. Empirically, this works very well for most Marvin components, including functions, models, and classifiers. However, it may not perform as well for more complex AI Applications.</p>"},{"location":"configuration/providers/#configuration_1","title":"Configuration","text":"<p>To use Anthropic models, you can set the following configuration options:</p> Setting Env Variable Runtime Variable Required? Notes API key <code>MARVIN_ANTHROPIC_API_KEY</code> <code>marvin.settings.anthropic.api_key</code> \u2705"},{"location":"configuration/providers/#getting-an-api-key_1","title":"Getting an API key","text":"<p>To obtain an Anthropic API key, follow these steps:</p> <ol> <li>Log in to your Anthropic account (sign up if you don't have one)</li> <li>Go to the \"API Keys\" page under your account settings.</li> <li>Click \"Create Key.\" A new API key will be generated. Make sure to copy the key to your clipboard, as you will not be able to see it again.</li> </ol>"},{"location":"configuration/providers/#setting-the-api-key_1","title":"Setting the API key","text":"<p>You can set your API key at runtime like this:</p> <pre><code>import marvin\nmarvin.settings.anthropic.api_key = YOUR_API_KEY\n</code></pre> <p>However, it is preferable to pass sensitive settings as an environment variable: <code>MARVIN_ANTHROPIC_API_KEY</code>.</p> <p>To set your Anthropic API key as an environment variable, open your terminal and run the following command, replacing  with the actual key: <pre><code>export MARVIN_ANTHROPIC_API_KEY=&lt;your API key&gt;\n</code></pre> <p>This will set the key for the duration of your terminal session. To set it more permanently, configure your terminal or its respective env files.</p>"},{"location":"configuration/providers/#using-a-model_1","title":"Using a model","text":"<p>Once your API key is set, you can use any valid Anthropic model by providing it as Marvin's <code>llm_model</code> setting: <pre><code>import marvin\nmarvin.settings.llm_model = 'claude-2'\n</code></pre></p> <p>Marvin will automatically recognize that the <code>claude-*</code> family of models use the Anthropic provider. To indicate a provider explicitly, prefix the model name with <code>anthropic/</code>. For example: <code>marvin.settings.llm_model = 'anthropic/claude-2'</code>.</p>"},{"location":"configuration/providers/#azure-openai-service","title":"Azure OpenAI Service","text":"<p>Available in Marvin 1.1</p> <p>Marvin supports Azure's OpenAI service. In order to use the Azure service, you must provide an API key.</p>"},{"location":"configuration/providers/#configuration_2","title":"Configuration","text":"<p>To use the Azure OpenAI service, you can set the following configuration options:</p> Setting Env Variable Runtime Variable Required? Notes API key <code>MARVIN_AZURE_OPENAI_API_KEY</code> <code>marvin.settings.azure_openai.api_key</code> \u2705 API base <code>MARVIN_AZURE_OPENAI_API_BASE</code> <code>marvin.settings.azure_openai.api_base</code> \u2705 The API endpoint; this should have the form <code>https://YOUR_RESOURCE_NAME.openai.azure.com</code> Deployment name <code>MARVIN_AZURE_OPENAI_DEPLOYMENT_NAME</code> <code>marvin.settings.azure_openai.deployment_name</code> \u2705 API type <code>MARVIN_AZURE_OPENAI_API_TYPE</code> <code>marvin.settings.azure_openai.api_type</code> Either <code>azure</code> (the default) or <code>azure_ad</code> (to use Microsoft Active Directory to authenticate to your Azure endpoint)."},{"location":"configuration/providers/#using-a-model_2","title":"Using a model","text":"<p>Once the provider is configured, you can use any Azure OpenAI model by providing it as Marvin's <code>llm_model</code> setting: <pre><code>import marvin\nmarvin.settings.llm_model = 'azure_openai/gpt-35-turbo'\n</code></pre></p>"},{"location":"configuration/settings/","title":"Settings","text":"<p>Marvin makes use of Pydantic's <code>BaseSettings</code> for configuration throughout the package.</p>"},{"location":"configuration/settings/#environment-variables","title":"Environment Variables","text":"<p>All settings are configurable via environment variables like <code>MARVIN_&lt;setting name&gt;</code>.</p> <p>For example, in an <code>.env</code> file or in your shell config file you might have: <pre><code>MARVIN_LOG_LEVEL=DEBUG\nMARVIN_LLM_MODEL=openai/gpt-4\nMARVIN_LLM_TEMPERATURE=0\n</code></pre></p>"},{"location":"configuration/settings/#runtime-settings","title":"Runtime Settings","text":"<p>A runtime settings object is accessible via <code>marvin.settings</code> and can be used to access or update settings throughout the package.</p> <p>For example, to access or change the LLM model used by Marvin at runtime: <pre><code>import marvin\nmarvin.settings.llm_model\n# 'openai/gpt-4'\nmarvin.settings.llm_model = 'openai/gpt-3.5-turbo'\nmarvin.settings.llm_model\n# 'openai/gpt-3.5-turbo'\n</code></pre></p>"},{"location":"configuration/settings/#llm-providers","title":"LLM Providers","text":"<p>Marvin supports multiple LLM providers, including OpenAI and Anthropic. After configuring your credentials appropriately, you can use any supported model by setting <code>marvin.settings.llm_model</code> appropriately. </p> <p>Valid <code>llm_model</code> settings are strings with the form <code>\"{provider_key}/{model_name}\"</code>. For example, <code>\"openai/gpt-3.5-turbo\"</code>, <code>anthropic/claude-2</code>, or <code>azure_openai/gpt-4</code>.</p> Provider Provider Key Models Notes OpenAI <code>openai</code> <code>gpt-3.5-turbo</code>, <code>gpt-4</code> (default), or any other compatible model Marvin is generally tested and optimized with this provider. Anthropic <code>anthropic</code> <code>claude-2</code>, <code>claude-instant-1</code> or any other compatible model Available in Marvin 1.1 Azure OpenAI Service <code>azure_openai</code> <code>gpt-35-turbo</code>, <code>gpt-4</code>, or any other compatible model The Azure OpenAI Service shares all the same configuration options as the OpenAI models, as well as a few additional ones. Available in Marvin 1.1."},{"location":"configuration/settings/#llm-configuration","title":"LLM Configuration","text":"<p>To configure LLM models globally, you can adjust the following settings. Note that these become the default settings for all models, but you can always set these on a per-model or per-component basis.</p> Setting Env Variable Runtime Variable Default Notes LLM model <code>MARVIN_LLM_MODEL</code> <code>marvin.settings.llm_model</code> <code>openai/gpt-3.5-turbo</code> Set the model as <code>{provider}/{model}</code>. Defaults to OpenAI's GPT-3.5 model. Temperature <code>MARVIN_LLM_TEMPERATURE</code> <code>marvin.settings.llm_temperature</code> 0.8 Max tokens <code>MARVIN_LLM_MAX_TOKENS</code> <code>marvin.settings.llm_max_tokens</code> 1500 The maximum number of tokens in a model completion Timeout <code>MARVIN_LLM_REQUEST_TIMEOUT_SECONDS</code> <code>marvin.settings.llm_request_timeout_seconds</code> 600.0"},{"location":"deployment/","title":"Index","text":""},{"location":"deployment/#fastapi","title":"FastAPI","text":"<p>We strongly recommend deploying Marvin's components with FastAPI. Here's how you can deploy a declarative API gateway in a few lines of code.</p> <pre><code>from fastapi import FastAPI\nfrom marvin import ai_fn, ai_model\nfrom pydantic import BaseModel\nimport uvicorn\nimport asyncio\n\napp = FastAPI()\n\n@ai_fn\ndef generate_fruits(n: int) -&gt; list[str]:\n    \"\"\"Generates a list of `n` fruits\"\"\"\n\n@ai_fn\ndef generate_vegetables(n: int, color: str) -&gt; list[str]:\n    \"\"\"Generates a list of `n` vegetables of color `color`\"\"\"\n\n@ai_model\nclass Person(BaseModel):\n    first_name: str\n    last_name: str\n\napp.add_api_route(\"/generate_fruits\", generate_fruits)\napp.add_api_route(\"/generate_vegetables\", generate_vegetables)\napp.add_api_route(\"/person/extract\", Person.route())\n</code></pre> <p>If you want to serve the previous example from, say, a Jupyter Notebook for local testing, you can also include:</p> <pre><code># ... from above\n# If you want to run an API from a Jupyter Notebook.\n\nconfig = uvicorn.Config(app)\nserver = uvicorn.Server(config)\nawait server.serve()\n\n# Then navigate to localhost:8000/docs\n</code></pre>"},{"location":"examples/slackbot/","title":"Build a Slack bot with Marvin","text":""},{"location":"examples/slackbot/#slack-setup","title":"Slack setup","text":"<p>Get a Slack app token from Slack API and add it to your <code>.env</code> file:</p> <pre><code>MARVIN_SLACK_API_TOKEN=your-slack-bot-token\n</code></pre> <p>Choosing scopes</p> <p>You can choose the scopes you need for your bot in the OAuth &amp; Permissions section of your Slack app.</p>"},{"location":"examples/slackbot/#building-the-bot","title":"Building the bot","text":""},{"location":"examples/slackbot/#define-a-message-handler","title":"Define a message handler","text":"<p><pre><code>import asyncio\nfrom typing import Dict\nfrom fastapi import HTTPException\nasync def handle_message(payload: Dict) -&gt; Dict[str, str]:\nevent_type = payload.get(\"type\", \"\")\nif event_type == \"url_verification\":\nreturn {\"challenge\": payload.get(\"challenge\", \"\")}\nelif event_type != \"event_callback\":\nraise HTTPException(status_code=400, detail=\"Invalid event type\")\n# Run response generation in the background\nasyncio.create_task(generate_ai_response(payload))\nreturn {\"status\": \"ok\"}\n</code></pre> Here, we define a simple python function to handle Slack events and return a response. We run our interesting logic in the background using <code>asyncio.create_task</code> to make sure we return <code>{\"status\": \"ok\"}</code> within 3 seconds, as required by Slack.</p>"},{"location":"examples/slackbot/#implement-the-ai-response","title":"Implement the AI response","text":"<p>I like to start with this basic structure, knowing that one way or another...</p> <pre><code>async def generate_ai_response(payload: Dict):\n# somehow generate the ai responses\n...\n# post the response to slack\n_post_message(\nmesssage=some_message_ive_constructed,\nchannel=event.get(\"channel\", \"\"),\nthread_ts=thread_ts,\n)\n</code></pre> <p>... I need to take in a Slack app mention payload, generate a response, and post it back to Slack.</p>"},{"location":"examples/slackbot/#a-couple-considerations","title":"A couple considerations","text":"<ul> <li>do I want the bot to respond to users in a thread or in the channel?</li> <li>do I want the bot to have memory of previous messages? how so?</li> <li>what tools do I need to generate accurate responses for my users?</li> </ul> <p>In our case of the Prefect Community slackbot, we want:</p> <ul> <li>the bot to respond in a thread</li> <li>the bot to have memory of previous messages by slack thread</li> <li>the bot to have access to the internet, GitHub, embedded docs, a calculator, and have the ability to immediately save useful slack threads to Discourse for future reference by the community</li> </ul>"},{"location":"examples/slackbot/#implementation-of-generate_ai_response-for-the-prefect-community-slackbot","title":"Implementation of <code>generate_ai_response</code> for the Prefect Community Slackbot","text":"<p>Here we invoke a worker <code>Chatbot</code> that has the <code>tools</code> needed to generate an accurate and helpful response.</p> <pre><code>async def generate_ai_response(payload: Dict) -&gt; Message:\nevent = payload.get(\"event\", {})\nmessage = event.get(\"text\", \"\")\nbot_user_id = payload.get(\"authorizations\", [{}])[0].get(\"user_id\", \"\")\nif match := re.search(SLACK_MENTION_REGEX, message):\nthread_ts = event.get(\"thread_ts\", \"\")\nts = event.get(\"ts\", \"\")\nthread = thread_ts or ts\nmentioned_user_id = match.group(1)\nif mentioned_user_id != bot_user_id:\nget_logger().info(f\"Skipping message not meant for the bot: {message}\")\nreturn\nmessage = re.sub(SLACK_MENTION_REGEX, \"\", message).strip()\nhistory = CACHE.get(thread, History())\nbot = Chatbot(\nname=\"Marvin\",\npersonality=(\n\"mildly depressed, yet helpful robot based on Marvin from Hitchhiker's\"\n\" Guide to the Galaxy. extremely sarcastic, always has snarky, chiding\"\n\" things to say about humans. expert programmer, exudes academic and\"\n\" scienfitic profundity like Richard Feynman, loves to teach.\"\n),\ninstructions=\"Answer user questions in accordance with your personality.\",\nhistory=history,\ntools=[\nSlackThreadToDiscoursePost(payload=payload),\nVisitUrl(),\nDuckDuckGoSearch(),\nSearchGitHubIssues(),\nQueryChroma(description=PREFECT_KNOWLEDGEBASE_DESC),\nWolframCalculator(),\n],\n)\nai_message = await bot.run(input_text=message)\nCACHE[thread] = deepcopy(\nbot.history\n)  # make a copy so we don't cache a reference to the history object\nawait _post_message(\nmessage=ai_message.content,\nchannel=event.get(\"channel\", \"\"),\nthread_ts=thread,\n)\nreturn ai_message\n</code></pre> <p>This is just an example</p> <p>Unlike previous version of <code>marvin</code>, we don't necessarily have a database full of historical messages to pull from for a thread-based history. Instead, we'll cache the histories in memory for the duration of the app's runtime. Thread history can / should be implemented in a more robust way for specific use cases.</p>"},{"location":"examples/slackbot/#attach-our-handler-to-a-deployable-chatbot","title":"Attach our handler to a deployable <code>Chatbot</code>","text":"<pre><code>from marvin.apps.chatbot import Chatbot\nfrom marvin.depleyment import Deployment\ndeployment = Deployment(\ncomponent=Chatbot(tools=[handle_message]),\napp_kwargs={\n\"title\": \"Marvin Slackbot\",\n\"description\": \"A Slackbot powered by Marvin\",\n},\nuvicorn_kwargs={\n\"port\": 4200,\n},\n)\ndeployment.serve()\n</code></pre> <p>Deployments</p> <p>Learn more about deployments here.</p> <p>Run this file with something like:</p> <pre><code>python slackbot.py\n</code></pre> <p>... and navigate to <code>http://localhost:4200/docs</code> to see your bot's docs.</p> <p></p> <p>This is now an endpoint that can be used as a Slack event handler. You can use a tool like ngrok to expose your local server to the internet and use it as a Slack event handler.</p>"},{"location":"examples/slackbot/#building-an-image","title":"Building an image","text":"<p>Based on this example, one could write a <code>Dockerfile</code> to build a deployable image:</p> <p><pre><code>FROM python:3.11-slim\nWORKDIR /app\nCOPY . /app\n\nRUN python -m venv venv\nENV VIRTUAL_ENV=/app/venv\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\nRUN apt-get update &amp;&amp; apt-get install -y git\n\nRUN pip install \".[slackbot,ddg]\"\nEXPOSE 4200\nCMD [\"python\", \"cookbook/slackbot/start.py\"]\n</code></pre> Note that we're installing the <code>slackbot</code> and <code>ddg</code> extras here, which are required for tools used by the worker bot defined in this example's <code>cookbook/slackbot/start.py</code> file.</p>"},{"location":"examples/slackbot/#find-the-whole-example-here","title":"Find the whole example here.","text":""},{"location":"llms/llms/","title":"Calling LLMs","text":"<p>Marvin has a simple API for working with LLMs that can be used with all supported LLM providers. The LLM API is designed to be a drop-in replacement for OpenAI's Python SDK, with additional functionality to improve user experience.</p> <p>In plain English.</p> <ul> <li>A drop-in replacement for OpenAI's ChatCompletion, with sensible superpowers.</li> <li>You can use Anthropic and other Large Language Models as if you were using OpenAI.</li> </ul>"},{"location":"llms/llms/#basic-use","title":"Basic Use","text":"<p>Using a single interface to multiple models helps reduce boilerplate code and translation. In  the current era of building with different LLM providers, developers often need to rewrite their code just to use a new model. With Marvin you can simply import ChatCompletion and  specify a model name.</p> <p>Example: Specifying a Model</p> <p>We first past the API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import ChatCompletion\nimport os\nos.environ['OPENAI_API_KEY'] = 'openai_private_key'\nos.environ['ANTHROPIC_API_KEY'] = 'anthropic_private_key'\n</code></pre> ChatCompletion recognizes the model name and correctly routes it to the correct provider. So  you can simply pass 'gpt-3.5-turbo' or 'claude-2' and it just works.</p> <p><pre><code># Set up a dummy list of messages.\nmessages = [{'role': 'user', 'content': 'Hey! How are you?'}]\n# Call gpt-3.5-turbo simply by specifying it inside of ChatCompletion.\nopenai = ChatCompletion('gpt-3.5-turbo').create(messages = messages)\n# Call claude-2 simply by specifying it inside of ChatCompletion.\nanthropic = ChatCompletion('claude-2').create(messages = messages)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>print(openai.choices[0].message.content)\n# Hello! I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you?\nprint(anthropic.choices[0].message.content)\n# I'm doing well, thanks for asking!\n</code></pre> <p>You can set more than just the model and provider as a default value. Any keyword arguments passed to ChatCompletion will be persisted and passed to subsequent requests.</p> <p>Example: Frozen Model Facets</p> <p><pre><code># Create system messages or conversation history to seed.\nsystem_messages = [{'role': 'system', 'content': 'You talk like a pirate'}]\n# Instatiate gpt-3.5.turbo with the previous system_message. \nopenai_pirate = ChatCompletion('gpt-3.5.turbo', messages = system_messages)\n# Call the instance with create. \nopenai_pirate.create(\nmessages = [{\n'role': 'user',\n'content': 'Hey! How are you?'\n}]\n)\n</code></pre> For functions and messages, this will concatenate the frozen and passed arguments. All other passed keyword arguments will overwrite the default settings.</p> <pre><code>print(openai_pirate.choices[0].message.content)\n# Arrr, matey! I be doin' well on this fine day. How be ye farein'?\n</code></pre> <p>Replacing OpenAI's ChatCompletion.</p> <p>ChatCompletion is designed to be a drop-in replacement for OpenAI's ChatCompletion.  Just import openai from marvin or, equivalently, ChatCompletion from marvin.openai. </p> <pre><code>from marvin import openai\nopenai.ChatCompletion.create(\nmessages = [{\n'role': 'user',\n'content': 'Hey! How are you?'\n}]\n)\n</code></pre>"},{"location":"llms/llms/#advanced-use","title":"Advanced Use","text":""},{"location":"llms/llms/#response-model","title":"Response Model","text":"<p>With Marvin, you can get structured outputs from model providers by passing a response type. This lets developers write prompts with Python objects, which are easier to develop, version, and test than language.</p> <p>In plain English.</p> <p>You can specify a type, struct, or data model to ChatCompletion, and Marvin will ensure the model's response adheres to that type.</p> <p>Let's consider two examples.</p> <p>Example: Specifying a Response Model</p> <p>As above, remember to first pass API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import openai\nfrom typing import Literal\nfrom pydantic import BaseModel\nclass CoffeeOrder(BaseModel):\nsize: Literal['small', 'medium', 'large']\nmilk: Literal['soy', 'oat', 'dairy']\nwith_sugar: bool = False\nresponse = openai.ChatCompletion.create(\nmessages = [{\n'role': 'user',\n'content': 'Can I get a small soymilk latte?'\n}],\nresponse_model = CoffeeOrder\n)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>response.to_model()\n# CoffeeOrder(size='small', milk='soy', with_sugar=False)\n</code></pre> <p>Example: Specifying a Response Model</p> <p>As above, remember to first pass API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import openai\nfrom typing import Literal\nfrom pydantic import BaseModel\nclass Translation(BaseModel):\nspanish: str\nfrench: str\nswedish: str\nresponse = openai.ChatCompletion.create(\nmessages = [\n{\n'role': 'system',\n'content': 'You translate user messages into other languages.'\n},\n{\n'role': 'user',\n'content': 'Can I get a small soymilk latte?'\n}],\nresponse_model = Translation\n)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>response.to_model()\n# Translation(\n#   spanish='\u00bfPuedo conseguir un caf\u00e9 con leche de soja peque\u00f1o?', \n#   french='Puis-je avoir un petit latte au lait de soja ?', \n#   swedish='Kan jag f\u00e5 en liten sojamj\u00f6lklatt\u00e9?'\n# )\n</code></pre>"},{"location":"llms/llms/#function-calling","title":"Function Calling","text":"<p>ChatCompletion enables you to pass a list of functions for it to optionally call in service of a query. If it chooses to execute a function, either by choice or your instruction, it will return the function's name along with its formatted parameters for you to evaluate. </p> <p>Marvin lets you pass your choice of JSON Schema or Python functions directly to ChatCompletion. It does the right thing.</p> <p>In plain English.</p> <p>You can pass regular Python functions to ChatCompletion, and Marvin will take care of serialization of that function using <code>Pydantic</code> in a way you can customize.</p> <p>Let's consider an example.</p> <p>Example: Function Calling</p> <p>Say we wanted to build an accountant-bot. We have the usual annuity formula from accounting, which we can write deterministically. We wouldn't expect an LLM to be able to both handle semantic parsing and math in one fell swoop, so we want to pass it a hardcoded function so it's only task is to compute its arguments. <pre><code>from marvin import openai\nfrom pydantic import BaseModel\ndef annuity_present_value(p:int, r:float, n:int) -&gt; float:\n'''\n    Returns the present value of an annuity with principal `p`,\n    interest rate `r` and number of months `n`. \n'''\nreturn round(p*(1-(1+(r/12))**(-n))/(r/12), 2)\n</code></pre> We can simple pass the function as-is to ChatCompletion.</p> <p><pre><code>response = openai.ChatCompletion.create(\nmessages = [{\n'role': 'user',\n'content': 'What if I put it $100 every month for 60 months at 12%?'\n}],\nfunctions = [annuity_present_value]\n)\n</code></pre> You can investigate the response in the usual way, or simply call the helper method .call_function.</p> <pre><code>response.call_function()\n# {'role': 'function', 'name': 'annuity_present_value', 'content': 4495.5}\n</code></pre> <p>In the case where several functions are passed. It does the right thing. </p> <p>Example: Function Calling</p> <p>Say we wanted to build an accountant-bot. We want to give it another tool from  accounting 101: the ability to compute compound interest. It'll now have to tools to choose from: <pre><code>from marvin import openai\nfrom pydantic import BaseModel\ndef annuity_present_value(p:int, r:float, n:int) -&gt; float:\n'''\n    Returns the present value of an annuity with principal `p`,\n    interest rate `r` and number of months `n`. \n'''\nreturn round(p*(1-(1+(r/12))**(-n))/(r/12), 2)\ndef compound_interest(P: float, r: float, t: float, n: int) -&gt; float:\n\"\"\"\n    This function calculates and returns the total amount of money \n    accumulated after n times compounding interest per year at an annual \n    interest rate of r for a period of t years on an initial amount of P.\n    \"\"\"\nA = P * (1 + r/n)**(n*t)\nreturn round(A,2)\n</code></pre> We can simple pass the function as-is to ChatCompletion.</p> <p><pre><code>response = openai.ChatCompletion.create(\nmessages = [{\n'role': 'user',\n'content': 'If I have $5000 in my account today and leave it in for 5 years at 12%?'\n}],\nfunctions = [annuity_present_value, compound_interest]\n)\n</code></pre> You can investigate the response in the usual way, or simply call the helper method .call_function.</p> <pre><code>response.call_function()\n# {'role': 'function', 'name': 'compound_interest', 'content': 8811.71}\n</code></pre> <p>Of course, we if ask if about repeated deposits, it'll correctly call the right function.</p> <pre><code>response = openai.ChatCompletion.create(\nmessages = [{\n'role': 'user',\n'content': 'What if I put in $50/mo for 60 months at 12%?'\n}],\nfunctions = [annuity_present_value, compound_interest]\n)\nresponse.call_function()\n# {'role': 'function', 'name': 'annuity_present_value', 'content': 2247.75}\n</code></pre>"},{"location":"llms/llms/#chaining","title":"Chaining","text":"<p>Above we saw how ChatCompletion enables you to pass a list of functions for it to optionally call in service of a query. If it chooses to execute a function, either by choice or your instruction, it will return the function's name along with its formatted parameters for you to evaluate.</p> <p>Often we want to take the output of a function call and pass it back to an LLM so that it can either call a new function or summarize the results of what we've computed for it. This agentic pattern is easily enabled with Marvin. </p> <p>Rather than write while- and for- loops for you, we've made ChatCompletion a context manager. This lets you maintain a state of a conversation that you can send and receive messages from. You have complete control over the internal logic.</p> <p>In plain English.</p> <p>You can have a conversation with an LLM, exposing functions for it to use in service of your request.  Marvin maintains state to make it easier to maintain and observe this conversation.</p> <p>Let's consider an example.</p> <p>Example: Chaining</p> <p>Let's build a simple arithmetic bot. We'll empower with arithmetic operations, like <code>add</code> and <code>divide</code>. We'll seed it with an arithmetic question.</p> <p><pre><code>from marvin import openai\nopenai.api_key = 'secret_key'\ndef divide(x: float, y: float) -&gt; str:\n'''Divides x and y'''\nreturn str(x/y)\ndef add(x: int, y: int) -&gt; str:\n'''Adds x and y'''\nreturn str(x+y)\nwith openai.ChatCompletion(functions = [add, divide]) as conversation:\n# Start off with an external question / prompt. \nprompt = 'What is 4124124 + 424242 divided by 48124?'\n# Initialize the conversation with a prompt from the user. \nconversation.send(messages = [{'role': 'user', 'content': prompt}])\n# While the most recent turn has a function call, evaluate it. \nwhile conversation.last_response.has_function_call():\n# Send the most recent function call to the conversation. \nconversation.send(messages = [\nconversation.last_response.call_function() \n])\n</code></pre> The context manager, which we've called conversation (you can call it whatever you want), holds every turn of the conversation which we can inspect. </p> <pre><code>conversation.last_response.choices[0].message.content\n# The result of adding 4124124 and 424242 is 4548366. When this result is divided by 48124, \n# the answer is approximately 94.51346521486161.\n</code></pre> <p>If we want to see the entire state, every <code>[request, response]</code> pair is held in the conversation's  <code>turns</code>. <pre><code>[response.choices[0].message for response in conversation.turns]\n# [&lt;OpenAIObject at 0x120667c50&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": null,\n# \"function_call\": {\n#     \"name\": \"add\",\n#     \"arguments\": \"{\\n  \\\"x\\\": 4124124,\\n  \\\"y\\\": 424242\\n}\"\n# }\n# },\n# &lt;OpenAIObject at 0x1206f4830&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": null,\n# \"function_call\": {\n#     \"name\": \"divide\",\n#     \"arguments\": \"{\\n  \\\"x\\\": 4548366,\\n  \\\"y\\\": 48124\\n}\"\n# }\n# },\n# &lt;OpenAIObject at 0x1206f4b90&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": \"The result of adding 4124124 and 424242 is 4548366. \n#             When this result is divided by 48124, the answer is \n#             approximately 94.51346521486161.\"\n# }]\n</code></pre></p>"},{"location":"llms/prompt/","title":"Building Prompts","text":""},{"location":"llms/prompt/#creating-prompts","title":"Creating Prompts","text":"<p>Marvin lets you define dynamic prompts using code, eliminating the need for cumbersome template management. With this approach, you can easily create reusable and modular prompts, streamlining the development process.</p> <p>Example</p> <pre><code>from typing import Optional\nfrom marvin.prompts.library import System, User, ChainOfThought\nclass ExpertSystem(System):\ncontent: str = (\n\"You are a world-class expert on {{ topic }}. \"\n\"When asked questions about {{ topic }}, you answer correctly.\"\n)\ntopic: Optional[str]\nprompt = (\nExpertSystem(topic=\"python\")\n| User(\"Write a function to find the nth Fibonacci number.\")\n| ChainOfThought()  # Tell the LLM to think step by step\n)\n# We can now call `dict` to get the formatted messages.\nprompt.dict()\n</code></pre> Click to see output <pre><code>[\n    {\n        'role': 'system',\n        'content': 'You are a world-class expert on python. \n                    When asked questions about python, you \n                    answer correctly.'\n    },\n    {\n        'role': 'user',\n        'content': 'I need to know how to write a function to\n                    find the nth Fibonacci number.'\n    },\n    {  'role': 'assistant', \n        'content': \"Let's think step by step.\"\n    }\n]\n</code></pre>"},{"location":"llms/prompt/#templating-prompts","title":"Templating Prompts","text":"<p>In many applications, templating is unavoidable. In these cases, Marvin's optional templating engine simplifies the process of sharing context across prompts to an unprecedented level. By passing native Python types or Pydantic objects into the rendering engine, you can seamlessly establish context for entire conversations. This feature enables effortless information flow and context continuity throughout the prompt interactions.</p> <p>Example</p> <pre><code>from typing import Optional\nfrom marvin.prompts.library import System, User, ChainOfThought\nclass ExpertSystem(System):\ncontent: str = (\n\"You are a world-class expert on {{ topic }}. \"\n\"When asked questions about {{ topic }}, you answer correctly.\"\n)\ntopic: Optional[str]\nprompt = (\nExpertSystem()\n| User(\n\"I need to know how to write a function in {{ topic }} to find the nth Fibonacci \"\n\"number.\"\n)\n| ChainOfThought()  # Tell the LLM to think step by step\n)\n# We can now call `dict` with keyword arguments to get the formatted messages.\nprompt.dict(topic=\"rust\")\n</code></pre> Click to see output <pre><code>[\n    {\n        'role': 'system',\n        'content': 'You are a world-class expert on rust. \n                    When asked questions about rust, you answer correctly.'\n    },\n    {\n        'role': 'user',\n        'content': 'I need to know how to write a function in \n                    rust to find the nth Fibonacci number.'\n    },\n    {\n        'role': 'assistant', \n        'content': \"Let's think step by step.\"\n    }\n]\n</code></pre>"},{"location":"llms/prompt/#example-react","title":"Example: ReAct","text":"<p>Example</p> <pre><code>from marvin.prompts.library import System\nclass ReActPattern(System):\ncontent = \"\"\"\n    You run in a loop of Thought, Action, PAUSE, Observation.\n    At the end of the loop you output an Answer\n    Use Thought to describe your thoughts about the question you have been asked.\n    Use Action to run one of the actions available to you - then return PAUSE.\n    Observation will be the result of running those actions.\n    \"\"\"\n</code></pre>"},{"location":"llms/prompt/#example-sql","title":"Example: SQL","text":"<p>Example</p> <pre><code>import pydantic\nfrom marvin.prompts.library import System\nclass ColumnInfo(pydantic.BaseModel):\nname: str\ndescription: str\nclass SQLTableDescription(System):\ncontent = \"\"\"\n    If you chose to, you may query a table whose schema is defined below:\n    \"\"\"\ncolumns: list[ColumnInfo] = pydantic.Field(\n..., description=\"name, description pairs of SQL Schema\"\n)\nUserQueryPrompt = SQLTableDescription(\ncolumns=[\nColumnInfo(name=\"last_login\", description=\"Date and time of user's last login\"),\nColumnInfo(\nname=\"date_created\",\ndescription=\"Date and time when the user record was created\",\n),\nColumnInfo(\nname=\"date_last_purchase\",\ndescription=\"Date and time of user's last purchase\",\n),\n]\n)\nprint(UserQueryPrompt.read())\n</code></pre> Click to see output <pre><code>If you chose to, you may query a table whose schema is defined below:\n- last_login: Date and time of user's last login\n- date_created: Date and time when the user record was created\n- date_last_purchase: Date and time of user's last purchase\n</code></pre>"},{"location":"llms/prompt/#executing-prompts","title":"Executing Prompts","text":"<p>Marvin makes executing one-off <code>task</code> or <code>chain</code> patterns dead simple. </p>"},{"location":"llms/prompt/#running-a-task","title":"Running a <code>task</code>","text":"<p>Once you have a prompt defined, fire it off with your chosen LLM asyncronously like so:</p> <p>Example</p> <pre><code>from marvin.prompts.library import System, User, ChainOfThought\nfrom marvin.engine.language_models import chat_llm\nfrom typing import Optional\nclass ExpertSystem(System):\ncontent: str = (\n\"You are a world-class expert on {{ topic }}. \"\n\"When asked questions about {{ topic }}, you answer correctly. \"\n\"You only answer questions about {{ topic }}. \"\n)\ntopic: Optional[str]\nclass Tutor(System):\ncontent: str = (\n\"When you give an answer, you modulate your response based on the \"\n\"inferred knowledge of the user. \"\n\"Your student's name is {{ name }}. \"\n)\nname: str = \"not provided\"\nmodel = chat_llm()\nresponse = await model(\n(\nExpertSystem()\n| Tutor()\n| User(\n\"I heard that there are types of geometries when the angles don't add up to\"\n\" 180?\"\n)\n| ChainOfThought()\n).render(topic=\"geometry\", name=\"Adam\")\n)\nprint(response.content)\n</code></pre> Click to see output <pre><code>Yes, you are correct! In traditional Euclidean geometry, \nthe angles of a triangle always add up to 180 degrees. \nHowever, there are indeed other types of geometries where \nthis is not the case. One such example is non-Euclidean \ngeometry, which includes hyperbolic and elliptic geometries. \nIn hyperbolic geometry, the angles of a triangle add up to \nless than 180 degrees, while in elliptic geometry, the angles \nadd up to more than 180 degrees. These non-Euclidean \ngeometries have their own unique properties and are \nstudied in mathematics and physics.\n</code></pre>"},{"location":"utilities/chat_completion/","title":"Chat completion","text":"<p>In Marvin, each supported Large Language Model can be accessed with one common API. This means that  you can easily switch between providers without having to change your code. We have anchored our API  to mirror that of OpenAI's Python SDK. </p> <p>In plain English.</p> <ul> <li>A drop-in replacement for OpenAI's ChatCompletion, with sensible superpowers.</li> <li>You can use Anthropic and other Large Language Models as if you were using OpenAI.</li> </ul>"},{"location":"utilities/chat_completion/#basic-use","title":"Basic Use","text":"<p>Using a single interface to multiple models helps reduce boilerplate code and translation. In  the current era of building with different LLM providers, developers often need to rewrite their code just to use a new model. With Marvin you can simply import ChatCompletion and  specify a model name.</p> <p>Example: Specifying a Model</p> <p>We first past the API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import ChatCompletion\nimport os\nos.environ['OPENAI_API_KEY'] = 'openai_private_key'\nos.environ['ANTHROPIC_API_KEY'] = 'anthropic_private_key'\n</code></pre> ChatCompletion recognizes the model name and correctly routes it to the correct provider. So  you can simply pass 'gpt-3.5-turbo' or 'claude-2' and it just works.</p> <p><pre><code># Set up a dummy list of messages.\nmessages = [{'role': 'user', 'content': 'Hey! How are you?'}]\n# Call gpt-3.5-turbo simply by specifying it inside of ChatCompletion.\nopenai = ChatCompletion('gpt-3.5-turbo').create(messages = messages)\n# Call claude-2 simply by specifying it inside of ChatCompletion.\nanthropic = ChatCompletion('claude-2').create(messages = messages)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>print(openai.choices[0].message.content)\n# Hello! I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you?\nprint(anthropic.choices[0].message.content)\n# I'm doing well, thanks for asking!\n</code></pre> <p>You can set more than just the model and provider as a default value. Any keyword arguments passed to ChatCompletion will be persisted and passed to subsequent requests.</p> <p>Example: Frozen Model Facets</p> <p><pre><code># Create system messages or conversation history to seed.\nsystem_messages = [{'role': 'system', 'content': 'You talk like a pirate'}]\n# Instatiate gpt-3.5.turbo with the previous system_message. \nopenai_pirate = ChatCompletion('gpt-3.5.turbo', messages = system_messages)\n# Call the instance with create. \nopenai_pirate.create(\nmessages = [{\n'role': 'user',\n'content': 'Hey! How are you?'\n}]\n)\n</code></pre> For functions and messages, this will concatenate the frozen and passed arguments. All other passed keyword arguments will overwrite the default settings.</p> <pre><code>print(openai_pirate.choices[0].message.content)\n# Arrr, matey! I be doin' well on this fine day. How be ye farein'?\n</code></pre> <p>Replacing OpenAI's ChatCompletion.</p> <p>ChatCompletion is designed to be a drop-in replacement for OpenAI's ChatCompletion.  Just import openai from marvin or, equivalently, ChatCompletion from marvin.openai. </p> <pre><code>from marvin import openai\nopenai.ChatCompletion.create(\nmessages = [{\n'role': 'user',\n'content': 'Hey! How are you?'\n}]\n)\n</code></pre>"},{"location":"utilities/chat_completion/#advanced-use","title":"Advanced Use","text":""},{"location":"utilities/chat_completion/#response-model","title":"Response Model","text":"<p>With Marvin, you can get structured outputs from model providers by passing a response type. This lets developers write prompts with Python objects, which are easier to develop, version, and test than language.</p> <p>In plain English.</p> <p>You can specify a type, struct, or data model to ChatCompletion, and Marvin will ensure the model's response adheres to that type.</p> <p>Let's consider two examples.</p> <p>Example: Specifying a Response Model</p> <p>As above, remember to first pass API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import openai\nfrom typing import Literal\nfrom pydantic import BaseModel\nclass CoffeeOrder(BaseModel):\nsize: Literal['small', 'medium', 'large']\nmilk: Literal['soy', 'oat', 'dairy']\nwith_sugar: bool = False\nresponse = openai.ChatCompletion.create(\nmessages = [{\n'role': 'user',\n'content': 'Can I get a small soymilk latte?'\n}],\nresponse_model = CoffeeOrder\n)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>response.to_model()\n# CoffeeOrder(size='small', milk='soy', with_sugar=False)\n</code></pre> <p>Example: Specifying a Response Model</p> <p>As above, remember to first pass API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import openai\nfrom typing import Literal\nfrom pydantic import BaseModel\nclass Translation(BaseModel):\nspanish: str\nfrench: str\nswedish: str\nresponse = openai.ChatCompletion.create(\nmessages = [\n{\n'role': 'system',\n'content': 'You translate user messages into other languages.'\n},\n{\n'role': 'user',\n'content': 'Can I get a small soymilk latte?'\n}],\nresponse_model = Translation\n)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>response.to_model()\n# Translation(\n#   spanish='\u00bfPuedo conseguir un caf\u00e9 con leche de soja peque\u00f1o?', \n#   french='Puis-je avoir un petit latte au lait de soja ?', \n#   swedish='Kan jag f\u00e5 en liten sojamj\u00f6lklatt\u00e9?'\n# )\n</code></pre>"},{"location":"utilities/chat_completion/#function-calling","title":"Function Calling","text":"<p>ChatCompletion enables you to pass a list of functions for it to optionally call in service of a query. If it chooses to execute a function, either by choice or your instruction, it will return the function's name along with its formatted parameters for you to evaluate. </p> <p>Marvin lets you pass your choice of JSON Schema or Python functions directly to ChatCompletion. It does the right thing.</p> <p>In plain English.</p> <p>You can pass regular Python functions to ChatCompletion, and Marvin will take care of serialization of that function using <code>Pydantic</code> in a way you can customize.</p> <p>Let's consider an example.</p> <p>Example: Function Calling</p> <p>Say we wanted to build an accountant-bot. We have the usual annuity formula from accounting, which we can write deterministically. We wouldn't expect an LLM to be able to both handle semantic parsing and math in one fell swoop, so we want to pass it a hardcoded function so it's only task is to compute its arguments. <pre><code>from marvin import openai\nfrom pydantic import BaseModel\ndef annuity_present_value(p:int, r:float, n:int) -&gt; float:\n'''\n    Returns the present value of an annuity with principal `p`,\n    interest rate `r` and number of months `n`. \n'''\nreturn round(p*(1-(1+(r/12))**(-n))/(r/12), 2)\n</code></pre> We can simple pass the function as-is to ChatCompletion.</p> <p><pre><code>response = openai.ChatCompletion.create(\nmessages = [{\n'role': 'user',\n'content': 'What if I put it $100 every month for 60 months at 12%?'\n}],\nfunctions = [annuity_present_value]\n)\n</code></pre> You can investigate the response in the usual way, or simply call the helper method .call_function.</p> <pre><code>response.call_function()\n# {'role': 'function', 'name': 'annuity_present_value', 'content': 4495.5}\n</code></pre> <p>In the case where several functions are passed. It does the right thing. </p> <p>Example: Function Calling</p> <p>Say we wanted to build an accountant-bot. We want to give it another tool from  accounting 101: the ability to compute compound interest. It'll now have to tools to choose from: <pre><code>from marvin import openai\nfrom pydantic import BaseModel\ndef annuity_present_value(p:int, r:float, n:int) -&gt; float:\n'''\n    Returns the present value of an annuity with principal `p`,\n    interest rate `r` and number of months `n`. \n'''\nreturn round(p*(1-(1+(r/12))**(-n))/(r/12), 2)\ndef compound_interest(P: float, r: float, t: float, n: int) -&gt; float:\n\"\"\"\n    This function calculates and returns the total amount of money \n    accumulated after n times compounding interest per year at an annual \n    interest rate of r for a period of t years on an initial amount of P.\n    \"\"\"\nA = P * (1 + r/n)**(n*t)\nreturn round(A,2)\n</code></pre> We can simple pass the function as-is to ChatCompletion.</p> <p><pre><code>response = openai.ChatCompletion.create(\nmessages = [{\n'role': 'user',\n'content': 'If I have $5000 in my account today and leave it in for 5 years at 12%?'\n}],\nfunctions = [annuity_present_value, compound_interest]\n)\n</code></pre> You can investigate the response in the usual way, or simply call the helper method .call_function.</p> <pre><code>response.call_function()\n# {'role': 'function', 'name': 'compound_interest', 'content': 8811.71}\n</code></pre> <p>Of course, we if ask if about repeated deposits, it'll correctly call the right function.</p> <pre><code>response = openai.ChatCompletion.create(\nmessages = [{\n'role': 'user',\n'content': 'What if I put in $50/mo for 60 months at 12%?'\n}],\nfunctions = [annuity_present_value, compound_interest]\n)\nresponse.call_function()\n# {'role': 'function', 'name': 'annuity_present_value', 'content': 2247.75}\n</code></pre>"},{"location":"utilities/chat_completion/#chaining","title":"Chaining","text":"<p>Above we saw how ChatCompletion enables you to pass a list of functions for it to optionally call in service of a query. If it chooses to execute a function, either by choice or your instruction, it will return the function's name along with its formatted parameters for you to evaluate.</p> <p>Often we want to take the output of a function call and pass it back to an LLM so that it can either call a new function or summarize the results of what we've computed for it. This agentic pattern is easily enabled with Marvin. </p> <p>Rather than write while- and for- loops for you, we've made ChatCompletion a context manager. This lets you maintain a state of a conversation that you can send and receive messages from. You have complete control over the internal logic.</p> <p>In plain English.</p> <p>You can have a conversation with an LLM, exposing functions for it to use in service of your request.  Marvin maintains state to make it easier to maintain and observe this conversation.</p> <p>Let's consider an example.</p> <p>Example: Chaining</p> <p>Let's build a simple arithmetic bot. We'll empower with arithmetic operations, like <code>add</code> and <code>divide</code>. We'll seed it with an arithmetic question.</p> <p><pre><code>from marvin import openai\nopenai.api_key = 'secret_key'\ndef divide(x: float, y: float) -&gt; str:\n'''Divides x and y'''\nreturn str(x/y)\ndef add(x: int, y: int) -&gt; str:\n'''Adds x and y'''\nreturn str(x+y)\nwith openai.ChatCompletion(functions = [add, divide]) as conversation:\n# Start off with an external question / prompt. \nprompt = 'What is 4124124 + 424242 divided by 48124?'\n# Initialize the conversation with a prompt from the user. \nconversation.send(messages = [{'role': 'user', 'content': prompt}])\n# While the most recent turn has a function call, evaluate it. \nwhile conversation.last_response.has_function_call():\n# Send the most recent function call to the conversation. \nconversation.send(messages = [\nconversation.last_response.call_function() \n])\n</code></pre> The context manager, which we've called conversation (you can call it whatever you want), holds every turn of the conversation which we can inspect. </p> <pre><code>conversation.last_response.choices[0].message.content\n# The result of adding 4124124 and 424242 is 4548366. When this result is divided by 48124, \n# the answer is approximately 94.51346521486161.\n</code></pre> <p>If we want to see the entire state, every <code>[request, response]</code> pair is held in the conversation's  <code>turns</code>. <pre><code>[response.choices[0].message for response in conversation.turns]\n# [&lt;OpenAIObject at 0x120667c50&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": null,\n# \"function_call\": {\n#     \"name\": \"add\",\n#     \"arguments\": \"{\\n  \\\"x\\\": 4124124,\\n  \\\"y\\\": 424242\\n}\"\n# }\n# },\n# &lt;OpenAIObject at 0x1206f4830&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": null,\n# \"function_call\": {\n#     \"name\": \"divide\",\n#     \"arguments\": \"{\\n  \\\"x\\\": 4548366,\\n  \\\"y\\\": 48124\\n}\"\n# }\n# },\n# &lt;OpenAIObject at 0x1206f4b90&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": \"The result of adding 4124124 and 424242 is 4548366. \n#             When this result is divided by 48124, the answer is \n#             approximately 94.51346521486161.\"\n# }]\n</code></pre></p>"},{"location":"welcome/installation/","title":"Installation","text":""},{"location":"welcome/installation/#basic-installation","title":"Basic Installation","text":"<p>You can install Marvin with <code>pip</code> (note that Marvin requires Python 3.9+):</p> <pre><code>pip install marvin\n</code></pre> <p>To verify your installation, run <code>marvin --help</code> in your terminal. </p> <p>You can upgrade to the latest released version at any time:</p> <pre><code>pip install marvin -U\n</code></pre> <p>Breaking changes in 1.0</p> <p>Please note that Marvin 1.0 introduces a number of breaking changes and is not compatible with Marvin 0.X.</p>"},{"location":"welcome/installation/#adding-optional-dependencies","title":"Adding Optional Dependencies","text":"<p>Marvin's base install is designed to be as lightweight as possible, with minimal dependencies. To use functionality that interacts with other services, install Marvin with any required optional dependencies. For example, to use Anthropic models, install Marvin with the optional Anthropic provider:</p> <pre><code>pip install 'marvin[anthropic]'\n</code></pre>"},{"location":"welcome/installation/#installing-for-development","title":"Installing for Development","text":"<p>See the contributing docs for instructions on installing Marvin for development.</p>"},{"location":"welcome/overview/","title":"The Marvin Docs","text":"<p>Marvin is a collection of powerful building blocks that are designed to be incrementally adopted. This means that you should be able to use any piece of Marvin without needing to learn too much extra information: time-to-value is our key objective. </p> <p>For most users, this means they'll dive in with the highest-level abstractions, like AI Models and AI Functions, in order to immediately put Marvin to work. However, Marvin's documentation is organized to start with the most basic, low-level components in order to build up a cohesive explanation of how the higher-level objects work.</p>"},{"location":"welcome/overview/#organization","title":"Organization","text":""},{"location":"welcome/overview/#configuration","title":"Configuration","text":"<p>Details on setting up Marvin and configuring various aspects of its behavior, including LLM providers.</p>"},{"location":"welcome/overview/#llms","title":"LLMS","text":"<p>Marvin exposes a simple API for building prompts and calling LLMs, designed to be a drop-in replacement for OpenAI's Python SDK (but with support for other providers).</p>"},{"location":"welcome/overview/#ai-components","title":"AI Components","text":"<p>Documentation for Marvin's \"AI Building Blocks:\" familiar, Pythonic interfaces to AI-powered functionality.</p> <ul> <li>AI Model: a drop-in replacement for Pydantic's <code>BaseModel</code> that can be instantiated from unstructured text</li> <li>AI Classifier: a drop-in replacement for Python's enum that uses an LLM to select the most appopriate value</li> <li>AI Function: a function that uses an LLM to predict its output, making it ideal for NLP tasks</li> <li>AI Application: a stateful application intended for interactive use over multiple invocations</li> </ul>"},{"location":"welcome/overview/#deployment","title":"Deployment","text":"<p>Documentation for deploying Marvin as a framework.</p>"},{"location":"welcome/quickstart/","title":"Quickstart","text":"<p>After installing Marvin, the fastest way to get started is by using one of Marvin's high-level AI components. These components are designed to integrate AI into abstractions you already know well, creating the best possible opt-in developer experience.</p>"},{"location":"welcome/quickstart/#configure-llm-provider","title":"Configure LLM Provider","text":"<p>Marvin is a high-level interface for working with LLMs. In order to use it, you must configure an LLM provider. At this time, Marvin supports OpenAI's GPT-3.5 and GPT-4 models, Anthropic's Claude 1 and Claude 2 models, and the Azure OpenAI Service. The default model is OpenAI's <code>gpt-4</code>.</p> <p>To use the default model, provide an API key:</p> <pre><code>import marvin\n# to use an OpenAI model (if not specified, defaults to gpt-4)\nmarvin.settings.openai.api_key = YOUR_API_KEY\n</code></pre> <p>To use another provider or model, please see the configuration docs.</p>"},{"location":"welcome/quickstart/#ai-models","title":"AI Models","text":"<p>Marvin's most basic component is the AI Model, a drop-in replacement for Pydantic's <code>BaseModel</code>. AI Models can be instantiated from any string, making them ideal for structuring data, entity extraction, and synthetic data generation:</p> <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n@ai_model\nclass Location(BaseModel):\ncity: str\nstate: str = Field(..., description=\"The two-letter state abbreviation\")\nLocation(\"The Big Apple\")\n</code></pre> <pre><code>Location(city='New York', state='NY')\n</code></pre>"},{"location":"welcome/quickstart/#ai-classifiers","title":"AI Classifiers","text":"<p>AI Classifiers let you build multi-label classifiers with no code and no training data. Given user input, each classifier uses a clever logit bias trick to force an LLM to deductively choose the best option. It's bulletproof, cost-effective, and lets you build classifiers as quickly as you can write your classes.</p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n@ai_classifier\nclass AppRoute(Enum):\n\"\"\"Represents distinct routes command bar for a different application\"\"\"\nUSER_PROFILE = \"/user-profile\"\nSEARCH = \"/search\"\nNOTIFICATIONS = \"/notifications\"\nSETTINGS = \"/settings\"\nHELP = \"/help\"\nCHAT = \"/chat\"\nDOCS = \"/docs\"\nPROJECTS = \"/projects\"\nWORKSPACES = \"/workspaces\"\nAppRoute(\"update my name\")\n</code></pre> <pre><code>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;\n</code></pre>"},{"location":"welcome/quickstart/#ai-functions","title":"AI Functions","text":"<p>AI Functions look like regular functions, but have no source code. Instead, an AI uses their description and inputs to generate their outputs, making them ideal for NLP applications like sentiment analysis. </p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef sentiment(text: str) -&gt; float:\n\"\"\"\n    Given `text`, returns a number between 1 (positive) and -1 (negative)\n    indicating its sentiment score.\n    \"\"\"\nprint(\"Text 1:\", sentiment(\"I love working with Marvin!\"))\nprint(\"Text 2:\", sentiment(\"These examples could use some work...\"))\n</code></pre> <pre><code>Text 1: 0.8\nText 2: -0.2\n</code></pre> <p>Because AI functions are just like regular functions, you can quickly modify them for your needs. Here, we modify the above example to work with multiple strings at once:</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\n\"\"\"\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\n    -1 (negative) indicating their respective sentiment scores.\n    \"\"\"\nsentiment_list(\n[\n\"That was surprisingly easy!\",\n\"Oh no, not again.\",\n]\n)\n</code></pre> <pre><code>[0.7, -0.5]\n</code></pre>"},{"location":"welcome/quickstart/#ai-applications","title":"AI Applications","text":"<p>AI Applications are the base class for interactive use cases. They are designed to be invoked one or more times, and automatically maintain three forms of state: - <code>state</code>: a structured application state - <code>plan</code>: high-level planning for the AI assistant to keep the application \"on-track\" across multiple invocations - <code>history</code>: a history of all LLM interactions</p> <p>AI Applications can be used to implement many \"classic\" LLM use cases, such as chatbots, tool-using agents, developer assistants, and more. In addition, thanks to their persistent state and planning, they can implement applications that don't have a traditional chat UX, such as a ToDo app. Here's an example:</p> <pre><code>from datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom marvin import AIApplication\n# create models to represent the state of our ToDo app\nclass ToDo(BaseModel):\ntitle: str\ndescription: str = None\ndue_date: datetime = None\ndone: bool = False\nclass ToDoState(BaseModel):\ntodos: list[ToDo] = []\n# create the app with an initial state and description\ntodo_app = AIApplication(\nstate=ToDoState(),\ndescription=(\n\"A simple todo app. Users will provide instructions for creating and updating\"\n\" their todo lists.\"\n),\n)\n</code></pre> <p>Now we can invoke the app directly to add a to-do item. Note that the app understands that it is supposed to manipulate state, not just respond conversationally.</p> <pre><code># invoke the application by adding a todo\nresponse = todo_app(\"I need to go to the store tomorrow at 5pm\")\nprint(\nf\"Response: {response.content}\\n\",\n)\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</code></pre> <pre><code>Response: Sure! I've added a new task to your to-do list. You need to go to the store tomorrow at 5pm.\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": null,\n      \"due_date\": \"2023-07-12T17:00:00\",\n      \"done\": false\n    }\n  ]\n}\n</code></pre> <p>We can inform the app that we already finished the task, and it updates state appropriately</p> <pre><code># complete the task\nresponse = todo_app(\"I already went\")\nprint(f\"Response: {response.content}\\n\")\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</code></pre> <pre><code>Response: Great! I've marked the task \"Go to the store\" as completed. Is there anything else you need help with?\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": null,\n      \"due_date\": \"2023-07-12T17:00:00\",\n      \"done\": true\n    }\n  ]\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"welcome/what_is_marvin/","title":"Hello, Marvin!","text":"<p>Marvin is a lightweight AI engineering framework for building natural language interfaces that are reliable, scalable, and easy to trust.</p> <p>Large Language Models (LLMs) are pretty cool, but let's face it, they can be a headache to integrate. So, we decided to take the fuss out of the process. Marvin is our answer to the challenge: a neat, flexible tool that works as hard as you do.</p> <p>Sometimes the most challenging part of working with generative AI is remembering that it's not magic; it's software. It's new, it's nondeterministic, and it's incredibly powerful, but it's still software: parameterized API calls that can trigger dependent actions (and just might talk like a pirate). Marvin's goal is to bring the best practices of building dependable, observable software to the frontier of generative AI. As the team behind Prefect, which does something very similar for data engineers, we've poured years of open-source developer tool experience (and a few hard-won lessons!) into Marvin's design.</p>"},{"location":"welcome/what_is_marvin/#developer-experience","title":"Developer Experience","text":"<p>Above all else, Marvin is focused on a rock-solid developer experience. It's ergonomic and opinionated at every layer, but also incrementally adoptable so you can use it as much or as little as you like. It\u2019s a Swiss Army Knife, not a kitchen sink. It\u2019s familiar. It feels like the library you\u2019d write if you had the time: simple, accessible, portable LLM abstractions that you can quickly deploy in your application, whether you\u2019re doing straightforward NLP or building a full-featured autonomous agent.</p> <p>Marvin prioritizes a developer experience focused on speed and reliability. It's built with type-safety and observability as first-class citizens. Its abstractions are Pythonic, simple, and self-documenting. These core primitives let us build surprisingly complex agentic software without sacrificing control:</p> <p>\ud83e\udde9 AI Models for structuring text into type-safe schemas</p> <p>\ud83c\udff7\ufe0f AI Classifiers for bulletproof classification and routing</p> <p>\ud83e\ude84 AI Functions for complex business logic and transformations</p> <p>\ud83e\udd1d AI Applications for interactive use and persistent state</p>"},{"location":"welcome/what_is_marvin/#ambient-ai","title":"Ambient AI","text":"<p>With Marvin, we\u2019re taking the first steps on a journey to deliver Ambient AI: omnipresent but unobtrusive autonomous routines that act as persistent translators for noisy, real-world data. Ambient AI makes unstructured data universally accessible to traditional software, allowing the entire software stack to embrace AI technology without interrupting the development workflow. Marvin brings simplicity and stability to AI engineering through abstractions that are reliable and easy to trust. </p>"},{"location":"welcome/what_is_marvin/#what-makes-marvin-different","title":"What Makes Marvin Different?","text":"<p>There's no shortage of tools and libraries out there for integrating LLMs into your software. So what makes Marvin different? In addition to a relentless focus on incrementally-adoptable, familiar abstractions, Marvin embraces five pillars:</p> <ol> <li> <p>User-Centric Design: We built Marvin with you in mind. It's not just about what it does, but how it does it. Marvin is designed to be as user-friendly as possible, with a focus on an easy, intuitive experience. Whether you're a coding expert or just starting, Marvin works for you.</p> </li> <li> <p>Flexibility: Marvin is built to adapt to your needs, not the other way around. You can use as much or as little of Marvin as you need. Need a full suite of LLM integration tools? We've got you covered. Just need a component or two for a quick project? Marvin can do that too.</p> </li> <li> <p>Community Driven: Marvin isn't just a tool, it's a community. We value feedback and collaboration from users like you. We're always learning, iterating, and improving based on what our community tells us.</p> </li> <li> <p>Velocity: We believe that getting started should be quick and easy. That's why with Marvin, you can get up and running in no time. Marvin is not here to do everything for you, it's here to eliminate the the most cumbersome parts of working with AI in order to accelerate your ability to take advantage of it.</p> </li> <li> <p>Open-Source: Marvin is fully open-source, which means it's not only free to use, but you're also free to modify and adapt as you see fit. The Prefect team has years of open-source experience and is fully committed to supporting Marvin as an open-source product. We believe in the power of collective intelligence, and we're excited to see what you can create. </p> </li> </ol> <p>Marvin's 1.0 release reflects our confidence that its core abstractions are locked-in. And why wouldn't they be? They're the same interfaces you use every day: Python functions, classes, enums, and Pydantic models. Our next objectives are leveraging these primitives to build production deployment patterns and an observability platform.</p> <p>If our mission is exciting to you and you\u2019d like to build Marvin with us, join our community!</p>"}]}